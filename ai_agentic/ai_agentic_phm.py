from typing_extensions import List, TypedDict
from datetime import datetime

from openai import OpenAI # TODO: Sá»­ dá»¥ng sau vá»›i multi-model language model
from langchain import hub 
from langchain.memory import ConversationBufferMemory
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma 
from langchain_openai import ChatOpenAI

if "Cáº¥u hÃ¬nh cÆ¡ sá»Ÿ dá»¯ liá»‡u ChromaDB":
	PERSIST_DIRECTORY = "/home/chwenjun225/Projects/Foxer/ai_agentic/chroma_db"
	COLLECTION_NAME = "foxconn_ai_research"
	EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
if "Táº¡o bá»™ nhá»› há»™i thoáº¡i":
	memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
if "Káº¿t ná»‘i ChromaDB Ä‘á»ƒ lÆ°u há»™i thoáº¡i vÃ  truy váº¥n RAG":
	embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
	vector_db = Chroma(
		persist_directory=PERSIST_DIRECTORY,
		embedding_function=embedding_model,
		collection_name=COLLECTION_NAME
	)

def get_llm(port, host, openai_api_key, model_name, temperature):
	"""Khá»Ÿi táº¡o mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n DeepSeek-R1 tá»« LlamaCpp-Server."""
	openai_api_base = "http://" + str(host) + ":" + str(port) 
	return ChatOpenAI(
		model=model_name, 
		openai_api_base=openai_api_base, 
		openai_api_key=openai_api_key, 
		temperature=temperature
	)

def save_chat_history_to_chroma_db(user_input, assistant_response):
	"""LÆ°u há»™i thoáº¡i vÃ o ChromaDB."""
	timestamp =  datetime.now().strftime("%Y-%m-%d_%H:%M:%S-%f")
	text_to_store = f"""[{timestamp}] ğŸ‘¨ User: {user_input}\n[{timestamp}] ğŸ¤– Assistant: {assistant_response}"""
	vector_db.add_texts(
		texts=[text_to_store],
		metadatas=[{"source": "chat_history"}]
	)

def retrieve_relevant_docs(query, num_docs=3):
	"""Truy váº¥n RAG tá»« ChromaDB."""
	docs = vector_db.similarity_search(query, k=num_docs)
	retrieved_texts = "\n".join([doc.page_content for doc in docs])
	return retrieved_texts
	
def planning_module(prompt_user, rag_output):
	"""Xá»­ lÃ½ thÃ´ng tin tá»« RAG Ä‘á»ƒ táº¡o prompt phÃ¹ há»£p cho LLM"""
	planning_prompt = f"""
User Question: {prompt_user}
Retrieved Knowledge from RAG:
{rag_output}

Generate the best response considering the retrieved knowledge. 
Keep it concise yet informative.
	"""
	return planning_prompt

if __name__ == "__main__":
	llm = get_llm(
		port=2026, 
		host="127.0.0.1", 
		openai_api_key="chwenjun225", 
		model_name="1_finetuned_DeepSeek-R1-Distill-Qwen-1.5B_finetune_CoT_ReAct", 
		temperature=0
	)
	# user_input = input(">>> ğŸ‘¨ User: ") 
	user_input = "What's the difference between revenue year of 2023 and 2024?"

	# ğŸ” Truy váº¥n RAG tá»« ChromaDB
	rag_output = retrieve_relevant_docs(query=user_input, num_docs=1)
	print(f">>> <rag_output>{rag_output}</rag_output>")

	# ğŸ§  Láº­p káº¿ hoáº¡ch pháº£n há»“i tá»« Planning Module
	planning_module_output = planning_module(user_input, rag_output)
	print(f">>> <planning_module>{planning_module_output}</planning_module>")

	# ğŸ¤– Gá»­i vÃ o LLM Ä‘á»ƒ nháº­n pháº£n há»“i
	response = llm.invoke([
		{"role": "system", "content": """
			You are a friendly and helpful assistant. 
			Your job is to answer human questions with care and detail. 
			Keep your answers short and concise when possible.
			"""}, 
		{"role": "user", "content": planning_module_output}
	])
	assistant_response = response.content
	print(f">>> ğŸ¤– Assistant:\n{assistant_response}")

	# ğŸ“ LÆ°u lá»‹ch sá»­ há»™i thoáº¡i vÃ o ChromaDB
	save_chat_history_to_chroma_db(user_input, assistant_response)
