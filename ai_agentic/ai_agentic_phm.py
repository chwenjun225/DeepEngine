import sys
# from typing_extensions import List, TypedDict
from datetime import datetime

from openai import OpenAI # TODO: Sá»­ dá»¥ng sau vá»›i multi-model language model
# from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

if "Khai bÃ¡o RAG-Prompt":
	prompt = ChatPromptTemplate.from_messages([
		("human", "You are an assistant for question-answering tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Use three sentences maximum and keep the answer concise. \
Question: {question} \
Context: {context} \
Answer:")
	]) 
if "Cáº¥u hÃ¬nh cÆ¡ sá»Ÿ dá»¯ liá»‡u ChromaDB":
	PERSIST_DIRECTORY = "/home/chwenjun225/Projects/Foxer/ai_agentic/chroma_db"
	COLLECTION_NAME = "foxconn_ai_research"
	EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
# if "Táº¡o bá»™ nhá»› há»™i thoáº¡i":
# 	memory = ConversationBufferMemory(
# 		memory_key="chat_history", 
# 		return_messages=True
# 	)
if "Káº¿t ná»‘i ChromaDB Ä‘á»ƒ lÆ°u há»™i thoáº¡i vÃ  truy váº¥n RAG":
	embedding_model = HuggingFaceEmbeddings(
		model_name=EMBEDDING_MODEL_NAME
	)
	vector_db = Chroma(
		persist_directory=PERSIST_DIRECTORY,
		embedding_function=embedding_model,
		collection_name=COLLECTION_NAME
	)
# if "CACHE":
# 	CACHE = {}

# def cached_llm_response(query):
# 	"""
# 	Náº¿u cÃ¢u há»i giá»‘ng 100% má»™t truy váº¥n cÅ©, láº¥y káº¿t quáº£ tá»« cache thay vÃ¬ gá»i LLM.
# 	"""
# 	# TODO: Cache Ä‘áº¿n má»™t giá»›i háº¡n nÃ o Ä‘Ã³ cáº§n Ä‘Æ°á»£c xÃ³a bá»›t.
# 	if query in CACHE: return CACHE[query]
# 	response = llm.invoke([{"role": "user", "content": query}])
# 	CACHE[query] = response.content
# 	return response.content

def get_llm(port, host, openai_api_key, model_name, temperature):
	"""
	Khá»Ÿi táº¡o mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n DeepSeek-R1 tá»« LlamaCpp-Server.
	"""
	openai_api_base = "http://" + str(host) + ":" + str(port) 
	return ChatOpenAI(
		model=model_name, 
		openai_api_base=openai_api_base, 
		openai_api_key=openai_api_key, 
		temperature=temperature
	)

def save_chat_history_to_chroma_db(user_input, assistant_response):
	"""
	LÆ°u há»™i thoáº¡i vÃ o ChromaDB.
	"""
	timestamp =  datetime.now().strftime("%Y-%m-%d_%H:%M:%S-%f")
	text_to_store = f"""[{timestamp}] ğŸ‘¨ User: {user_input}\n[{timestamp}] ğŸ¤– Assistant: {assistant_response}"""
	vector_db.add_texts(
		texts=[text_to_store],
		metadatas=[{"source": "chat_history"}]
	)

def rag(query, num_retrieved_docs=3):
# TODO: 
	# 1. ThÃªm RAG-Prompt ("""Náº¿u khÃ´ng tÃ¬m tháº¥y hÃ£y nÃ³i ráº±ng tÃ´i khÃ´ng biáº¿t""")
	# 2. Cáº£i Thiá»‡n Tá»‘c Äá»™ Truy Váº¥n RAG
	# Hiá»‡n táº¡i: Há»‡ thá»‘ng tÃ¬m kiáº¿m trong ChromaDB báº±ng similarity_search(), cÃ³ thá»ƒ cháº­m khi dá»¯ liá»‡u lá»›n.
	# âœ… Giáº£i phÃ¡p: DÃ¹ng FAISS Index hoáº·c Hybrid Search (tÃ¬m kiáº¿m káº¿t há»£p tá»« khÃ³a + vector).
	"""
	Truy váº¥n RAG tá»« ChromaDB.
	"""
# 	prompt = ChatPromptTemplate.from_messages([
# 		("human", """
# You are an assistant for question-answering tasks. 
# Use the following pieces of retrieved context to answer the question. 
# If you don't know the answer, just say that you don't know.
# Use three sentences maximum and keep the answer concise. 
# Question: {question} 
# Context: {context} 
# Answer:"""),
# 	])
	retriever_docs = vector_db.similarity_search(query, k=num_retrieved_docs)
	retrieved_texts = "\n".join([retriever_doc.page_content for retriever_doc in retriever_docs])
	return retrieved_texts

def planning_module(prompt_user, rag_output):
# TODO:
	# Hiá»‡n táº¡i: Planning chá»‰ ghÃ©p prompt_user + rag_output.
	# âœ… Giáº£i phÃ¡p:

	# DÃ¹ng prompt templates chuyÃªn biá»‡t Ä‘á»ƒ tá»‘i Æ°u pháº£n há»“i.
	# Sá»­ dá»¥ng Chain of Thought (CoT) hoáº·c ReAct Ä‘á»ƒ tÄƒng cÆ°á»ng reasoning.
	# Náº¿u dÃ¹ng LLM máº¡nh hÆ¡n, cÃ³ thá»ƒ Ã¡p dá»¥ng Multi-Step Planning.
	"""
	Xá»­ lÃ½ thÃ´ng tin tá»« RAG Ä‘á»ƒ táº¡o prompt phÃ¹ há»£p cho LLM.
	"""
	planning_prompt = prompt.invoke({
		"question": prompt_user,
		"context": rag_output
	})
# 	planning_prompt = f"""
# User Question: {prompt_user}
# Retrieved Knowledge from RAG:
# {rag_output}

# Generate the best response considering the retrieved knowledge. 
# Keep it concise yet informative.
# 	"""
	return planning_prompt

if __name__ == "__main__":
	with open("chroma_logs.txt", "a") as f:
		sys.stdout = f  # Chuyá»ƒn táº¥t cáº£ print() vÃ o file
		llm = get_llm(
			port=2026, 
			host="127.0.0.1", 
			openai_api_key="chwenjun225", 
			model_name="1_finetuned_DeepSeek-R1-Distill-Qwen-1.5B_finetune_CoT_ReAct", 
			temperature=0
		)
		# user_input = input(">>> ğŸ‘¨ User: ") 
		user_input = "What's the difference between revenue year of 2023 and 2024?"

		# ğŸ” Truy váº¥n RAG tá»« ChromaDB
		rag_output = rag(query=user_input, num_retrieved_docs=1)
		print(f">>> <rag_output>{rag_output}</rag_output>")

		# ğŸ§  Láº­p káº¿ hoáº¡ch pháº£n há»“i tá»« Planning Module
		planning_module_output = planning_module(user_input, rag_output)
		print(f">>> <planning_module>{planning_module_output}</planning_module>")

		# ğŸ¤– Gá»­i vÃ o LLM Ä‘á»ƒ nháº­n pháº£n há»“i
		response_template = ChatPromptTemplate.from_messages([
			("system", "You are a friendly and helpful assistant. \
	Your job is to answer human questions with care and detail. \
	Keep your answers short and concise when possible."),
			("user", "{input}")
		])
		formatted_response_template = response_template.invoke({"input": planning_module_output})
	# 	response = llm.invoke([
	# 		{"role": "system", "content": " You are a friendly and helpful assistant. \
	# Your job is to answer human questions with care and detail. \
	# Keep your answers short and concise when possible."}, 
	# 		{"role": "user", "content": planning_module_output}
	# 	])
		response = llm.invoke(formatted_response_template)
		assistant_response = response.content
		print(f">>> ğŸ¤– Assistant:\n{assistant_response}")

		# ğŸ“ LÆ°u lá»‹ch sá»­ há»™i thoáº¡i vÃ o ChromaDB
		save_chat_history_to_chroma_db(user_input, assistant_response)
		sys.stdout = sys.__stdout__  # Reset láº¡i stdout vá» máº·c Ä‘á»‹nh
