VllmServer_params:  # Configuration for the VLLM server
  gpu_memory_utilization: 0.9  # GPU memory utilization, 0.9 means 90%
  dtype: "float16"  # Precision of the model's computations
  tensor_parallel_size: 4  # Number of tensor parallel units
  tokenizer_mode: "auto"  # Automatically select tokenizer mode

  sampling_params: # Parameters for controlling generation
    n: 1
    best_of: 1
    presence_penalty: 1.0
    frequency_penalty: 0.0
    temperature: 0.8
    top_p: 0.8
    top_k: -1
    stop: null
    stop_token_ids: null
    ignore_eos: false
    max_tokens: 500
    logprobs: null
    prompt_logprobs: null
    skip_special_tokens: True


metric_VllmServer_params:  # Configuration for the VLLM server
  gpu_memory_utilization: 0.9  # GPU memory utilization, 0.9 means 90%
  dtype: "float16"  # Precision of the model's computations
  tensor_parallel_size: 4  # Number of tensor parallel units
  tokenizer_mode: "auto"  # Automatically select tokenizer mode

  sampling_params: # Parameters for controlling generation
    n: 1
    best_of: 1
    presence_penalty: 1.0
    frequency_penalty: 0.0
    temperature: 0.8
    top_p: 0.8
    top_k: -1
    stop: null
    stop_token_ids: null
    ignore_eos: false
    max_tokens: 500
    logprobs: null
    prompt_logprobs: null
    skip_special_tokens: True