Yes, there are several multimodal large language models (LLMs) that can process both text and images, and are suitable for running on a local machine equipped with an NVIDIA RTX 4090 GPU. Here are some notable options:

:::contextList
【{"image_fetch": "MiniCPM-o2.6 model"}】
**MiniCPM-o2.6**  
An advanced multimodal LLM with 8 billion parameters, capable of handling vision, speech, and text processing. It offers features like real-time speech conversations, multimodal live streaming, and optical character recognition (OCR). citeturn0search0
:::

:::contextList
【{"image_fetch": "VILA model"}】
**VILA (Vision Language Models)**  
A family of open VLMs optimized for efficient video and multi-image understanding. VILA models are designed to balance efficiency and accuracy, making them suitable for various multimodal applications. citeturn0search2
:::

:::contextList
【{"image_fetch": "LLaVA model"}】
**LLaVA (Large Language and Vision Assistant)**  
A project that enables multimodal AI by integrating language and visual understanding. LLaVA can interpret text, analyze images, and provide detailed descriptions, making it a versatile tool for various applications. citeturn0search4
:::

:::contextList
【{"image_fetch": "Yi-34B-Chat model"}】
**Yi-34B-Chat**  
A large language model designed for multimodal tasks, capable of running on hardware setups like an A800 GPU or a configuration with four RTX 4090 GPUs. It supports various inference tasks involving text and images. citeturn0search7
:::

:::contextList
【{"image_fetch": "Llama 3.2 model"}】
**Llama 3.2**  
Meta's open-source AI model that processes both images and text. It's designed to help developers create enhanced AI applications, such as augmented reality apps, visual search engines, and document analysis tools. citeturn0news11
:::

Before deploying any of these models, ensure your system meets the necessary hardware and software requirements. Additionally, consider the specific needs of your application to select the most appropriate model. 