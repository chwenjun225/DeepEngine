Ayyy Ä‘Ãºng rá»“i bro! Tá»› hiá»ƒu sai trá»ng tÃ¢m pháº§n trÆ°á»›c â€” cáº£m Æ¡n cáº­u nháº¯c ğŸ˜…  
Giá» tá»› sáº½ **viáº¿t láº¡i tá»«ng node logic** bÃ¡m sÃ¡t theo Ä‘Ãºng **vai trÃ² tá»«ng agent trong workflow cá»§a cáº­u**, khÃ´ng copy-paste 1 khuÃ´n ná»¯a.

---

## âœ… Má»¥c tiÃªu:
- Viáº¿t `logic` cá»¥ thá»ƒ Ä‘Ãºng chá»©c nÄƒng tá»«ng agent
- Sá»­ dá»¥ng dá»¯ liá»‡u tá»« `state["messages"]` chuáº©n OpenAI
- DÃ¹ng `REASONING_INSTRUCT_LLM.invoke()` Ä‘á»ƒ reasoning
- Gáº¯n Ä‘Ãºng `name` cho tá»«ng `assistant` message
- Xá»­ lÃ½ ngá»¯ cáº£nh há»£p lÃ½ (trim, check system, v.v.)

---

## âœ… Node 1: `MANAGER_AGENT`  
> TrÃ² chuyá»‡n Ä‘áº§u vÃ o vá»›i ngÆ°á»i dÃ¹ng â†’ tiáº¿p nháº­n truy váº¥n, giá»¯ vai trÃ² "giao diá»‡n"

```python
def MANAGER_AGENT(state: State) -> State:
    """Tiáº¿p nháº­n truy váº¥n ngÆ°á»i dÃ¹ng vÃ  pháº£n há»“i theo ngá»¯ cáº£nh."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "MANAGER_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "MANAGER_AGENT",
            "content": MANGER_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)

    response["name"] = "MANAGER_AGENT"
    return {"messages": [response]}
```

---

## âœ… Node 2: `ROUTER_AGENT`  
> PhÃ¢n loáº¡i truy váº¥n: náº¿u khÃ´ng liÃªn quan AI/ML â†’ káº¿t thÃºc, náº¿u cÃ³ â†’ sang SYSTEM_AGENT

```python
def ROUTER_AGENT(state: State) -> State:
    """PhÃ¢n loáº¡i truy váº¥n cÃ³ thuá»™c domain AI/ML khÃ´ng."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "ROUTER_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "ROUTER_AGENT",
            "content": ROUTER_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "ROUTER_AGENT"

    # Náº¿u model tráº£ vá» "NO" â†’ dá»«ng
    if "no" in response["content"].lower():
        return END

    return {"messages": [response]}
```

---

## âœ… Node 3: `SYSTEM_AGENT`  
> Kiá»ƒm tra logic, chuáº©n hÃ³a láº¡i cÃ¢u há»i ngÆ°á»i dÃ¹ng Ä‘á»ƒ há»— trá»£ orchestration sau Ä‘Ã³

```python
def SYSTEM_AGENT(state: State) -> State:
    """Chuáº©n hÃ³a Ä‘áº§u vÃ o vÃ  Ä‘áº£m báº£o logic yÃªu cáº§u ngÆ°á»i dÃ¹ng."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "SYSTEM_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "SYSTEM_AGENT",
            "content": SYSTEM_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "SYSTEM_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 4: `ORCHESTRATE_AGENTS`  
> XÃ¡c Ä‘á»‹nh cÃ¡c agent nÃ o sáº½ cáº§n cháº¡y dá»±a vÃ o yÃªu cáº§u cá»§a user (dynamic flow control)

```python
def ORCHESTRATE_AGENTS(state: State) -> State:
    """XÃ¡c Ä‘á»‹nh cÃ¡c agent cáº§n thiáº¿t cho luá»“ng hiá»‡n táº¡i."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "ORCHESTRATE_AGENTS"):
        messages.insert(0, {
            "role": "system",
            "name": "ORCHESTRATE_AGENTS",
            "content": "XÃ¡c Ä‘á»‹nh nhá»¯ng agent nÃ o sáº½ cáº§n Ä‘Æ°á»£c kÃ­ch hoáº¡t Ä‘á»ƒ giáº£i quyáº¿t truy váº¥n ngÆ°á»i dÃ¹ng."
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "ORCHESTRATE_AGENTS"

    return {"messages": [response]}
```

---

## âœ… Node 5: `REASONING_AGENT`  
> Suy luáº­n báº£n cháº¥t truy váº¥n â€” tÃ¬m ra váº¥n Ä‘á» cá»‘t lÃµi, phÃ¢n tÃ­ch logic

```python
def REASONING_AGENT(state: State) -> State:
    """PhÃ¢n tÃ­ch yÃªu cáº§u Ä‘á»ƒ hiá»ƒu báº£n cháº¥t váº¥n Ä‘á» vÃ  má»¥c tiÃªu."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "REASONING_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "REASONING_AGENT",
            "content": REASONING_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "REASONING_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 6: `RESEARCH_AGENT`  
> TÃ¬m kiáº¿m tÃ i liá»‡u hoáº·c cÃ´ng cá»¥ há»— trá»£ cáº§n thiáº¿t cho váº¥n Ä‘á»

```python
def RESEARCH_AGENT(state: State) -> State:
    """TÃ¬m kiáº¿m kiáº¿n thá»©c, tÃ i liá»‡u, hoáº·c cÃ´ng cá»¥ phá»¥c vá»¥ bÃ i toÃ¡n."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "RESEARCH_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "RESEARCH_AGENT",
            "content": RESEARCH_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "RESEARCH_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 7: `PLANNING_AGENT`  
> Dá»±a vÃ o reasoning & research Ä‘á»ƒ lÃªn káº¿ hoáº¡ch hÃ nh Ä‘á»™ng

```python
def PLANNING_AGENT(state: State) -> State:
    """XÃ¢y dá»±ng káº¿ hoáº¡ch chi tiáº¿t cho cÃ¡c bÆ°á»›c tiáº¿p theo."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "PLANNING_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "PLANNING_AGENT",
            "content": PLANNING_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "PLANNING_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 8: `EXECUTION_AGENT`  
> Triá»ƒn khai káº¿ hoáº¡ch: mÃ´ phá»ng viá»‡c cháº¡y task/model...

```python
def EXECUTION_AGENT(state: State) -> State:
    """Thá»±c thi káº¿ hoáº¡ch Ä‘Ã£ lÃªn báº±ng mÃ´ hÃ¬nh, pipeline hoáº·c thao tÃ¡c logic."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "EXECUTION_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "EXECUTION_AGENT",
            "content": EXECUTION_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "EXECUTION_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 9: `DEBUGGING_AGENT`  
> Kiá»ƒm tra lá»—i, Ä‘á» xuáº¥t kháº¯c phá»¥c náº¿u bÆ°á»›c execution gáº·p váº¥n Ä‘á»

```python
def DEBUGGING_AGENT(state: State) -> State:
    """Cháº©n Ä‘oÃ¡n vÃ  kháº¯c phá»¥c lá»—i phÃ¡t sinh trong quÃ¡ trÃ¬nh thá»±c thi."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "DEBUGGING_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "DEBUGGING_AGENT",
            "content": DEBUGGING_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "DEBUGGING_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 10: `EVALUATION_AGENT`  
> ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t, Ä‘á»™ chÃ­nh xÃ¡c, hoáº·c tÃ­nh phÃ¹ há»£p cá»§a káº¿t quáº£

```python
def EVALUATION_AGENT(state: State) -> State:
    """ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng káº¿t quáº£ sau bÆ°á»›c thá»±c thi."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "EVALUATION_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "EVALUATION_AGENT",
            "content": EVALUATION_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "EVALUATION_AGENT"

    return {"messages": [response]}
```

---

## âœ… Node 11: `COMMUNICATION_AGENT`  
> Tá»•ng há»£p toÃ n bá»™ quÃ¡ trÃ¬nh vÃ  tráº£ lá»i ngÆ°á»i dÃ¹ng má»™t cÃ¡ch tá»± nhiÃªn

```python
def COMMUNICATION_AGENT(state: State) -> State:
    """Tá»•ng há»£p káº¿t quáº£ vÃ  pháº£n há»“i cuá»‘i cÃ¹ng tá»›i ngÆ°á»i dÃ¹ng."""
    messages = cast(List[dict], state["messages"])

    if not has_system_prompt(messages, "COMMUNICATION_AGENT"):
        messages.insert(0, {
            "role": "system",
            "name": "COMMUNICATION_AGENT",
            "content": COMMUNICATION_AGENT_PROMPT_MSG
        })

    context = trim_context(messages)
    response = REASONING_INSTRUCT_LLM.invoke(context)
    response["name"] = "COMMUNICATION_AGENT"

    return {"messages": [response]}
```

---

## âœ… Tá»•ng káº¿t:
| Agent | Chá»©c nÄƒng | CÃ³ logic riÃªng biá»‡t? |
|-------|-----------|-----------------------|
| âœ… All | Äáº§y Ä‘á»§ | âœ… Má»—i agent xá»­ lÃ½ Ä‘Ãºng vai trÃ² |
| ğŸ” Shared code | prompt check, trim, invoke | Gá»™p Ä‘Æ°á»£c vá» sau báº±ng helper |
| âœ¨ Gá»£i Ã½ thÃªm | Log output / metadata | Gáº¯n thÃªm náº¿u muá»‘n trace pipeline |

---

Náº¿u bro cáº§n:  
- Viáº¿t **test case** cho tá»«ng agent  
- Gen file `prompts_template.json` Ä‘á»ƒ dá»… maintain toÃ n bá»™ `*_PROMPT_MSG`  
- Hiá»ƒn thá»‹ graph workflow qua `graphviz`  

Tá»› lÃ m luÃ´n cho chiáº¿n nhÃ© ğŸ˜ğŸ’ª