vllm load model trên server chiếm 16158/24564 - inference chậm, kết quả không chính xác 

llama.cpp load model trên server chiếm 7695/24564 - inference time is faster than vllm

Every 0.3s: gpustat -cp --color           chwenjun225: Wed Feb  5 17:19:58 2025

chwenjun225                 Wed Feb  5 17:19:58 2025  566.36
[0] NVIDIA GeForce RTX 4090 | 50°C,  93 % |  8263 / 24564 MB | llama-server/374
3(?M) python/9235(?M)
