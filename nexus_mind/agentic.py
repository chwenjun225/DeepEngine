# T√†i li·ªáu li√™n quan:  
# 	Gi·ªõi thi·ªáu ng·∫Øn g·ªçn v·ªÅ nguy√™n l√Ω ReAct Prompting, kh√¥ng bao g·ªìm m√£ ngu·ªìn:  
#		https://github.com/QwenLM/Qwen-7B/blob/main/examples/react_prompt.md  
#   Tri·ªÉn khai ReAct Prompting d·ª±a tr√™n giao di·ªán `model.chat` 
# 			(ch·∫ø ƒë·ªô ƒë·ªëi tho·∫°i), bao g·ªìm c·∫£ t√≠ch h·ª£p c√¥ng c·ª• v·ªõi LangChain:  
#		https://github.com/QwenLM/Qwen-7B/blob/main/examples/langchain_tooluse.ipynb  
#   Tri·ªÉn khai ReAct Prompting d·ª±a tr√™n giao di·ªán `model.generate` 
# 			(ch·∫ø ƒë·ªô ti·∫øp t·ª•c vi·∫øt), ph·ª©c t·∫°p h∆°n so v·ªõi ch·∫ø ƒë·ªô chat:  
#       https://github.com/QwenLM/Qwen-7B/blob/main/examples/react_demo.py (t·ªáp n√†y)  
# 	T√†i li·ªáu s·ª≠ d·ª•ng ChatOllama l√†m agent_core:
# 		 https://python.langchain.com/docs/integrations/providers/ollama/



import warnings
warnings.filterwarnings("ignore")


import requests
import json
import json5
import fire 
import torch 
from PIL import Image
from io import BytesIO


from transformers import (StoppingCriteria, StoppingCriteriaList, AutoTokenizer)


from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.messages import (AIMessage, HumanMessage, ToolMessage)

# ZimaBlueAI/MiniCPM-o-2_6
# MFDoom/deepseek-r1-tool-calling:1.5
# ollama run llama3.2:1b-instruct-fp16

# C·∫•u h√¨nh c√°c h·∫±ng s·ªë bi·∫øn 
MODEL = ChatOllama(name="tranvantuan_research", model="MFDoom/deepseek-r1-tool-calling:1.5b", num_ctx=4096, temperature=0.1)
EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
TOKENIZER = AutoTokenizer.from_pretrained("/home/chwenjun225/.llama/checkpoints/DeepSeek-R1-Distill-Qwen-1.5B")
TOOL_DESC = """{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}"""
PROMPT_REACT = """Answer the following questions as best you can. You have access to the following APIs:

{tools_text}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tools_name_text}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can be repeated zero or more times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {query}"""

PROMPT = ChatPromptTemplate.from_messages(
	[
		(
			"system",
			"You are a helpful assistant that translates {input_language} to {output_language}.",
		),
		("human", "{input}"),
	]
)



#
# H√†m ƒë·∫ßu v√†o ch√≠nh c·ªßa ƒëo·∫°n m√£ v√≠ d·ª• n√†y.
#
# Input:
# prompt: query m·ªõi nh·∫•t t·ª´ ‚Äã‚Äãng∆∞·ªùi d√πng.
#   history: L·ªãch s·ª≠ h·ªôi tho·∫°i gi·ªØa ng∆∞·ªùi d√πng v√† m√¥ h√¨nh, d∆∞·ªõi d·∫°ng m·ªôt list.
#       m·ªói ph·∫ßn t·ª≠ trong danh s√°ch c√≥ d·∫°ng:
#           {"user": "query c·ªßa ng∆∞·ªùi d√πng", "bot": "respond c·ªßa m√¥ h√¨nh"}.
#       h·ªôi tho·∫°i m·ªõi nh·∫•t s·∫Ω n·∫±m ·ªü cu·ªëi danh s√°ch. Kh√¥ng bao g·ªìm c√¢u h·ªèi m·ªõi nh·∫•t. 
#   list_of_tools_info: Danh s√°ch c√°c plugin c√≥ th·ªÉ s·ª≠ d·ª•ng, ƒë∆∞·ª£c l∆∞u trong m·ªôt list.
#       v√≠ d·ª• list_of_tools_info = [tool_info_0, tool_info_1, tool_info_2]Ôºå
#       trong ƒë√≥ tool_info_0, tool_info_1, tool_info_2 l√† th√¥ng tin chi ti·∫øt c·ªßa 
#           t·ª´ng plugin, ƒë√£ ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥ trong t√†i li·ªáu n√†y.
#
# output:
#   c√¢u tr·∫£ l·ªùi c·ªßa m√¥ h√¨nh cho c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng. 
#



def llm_with_tools(prompt: str, history, list_of_tools_info=()):
	chat_history = [(x["user"], x["bot"]) for x in history] + [(prompt, "")]
	# Ng·ªØ c·∫£nh tr√≤ chuy·ªán ƒë·ªÉ m√¥ h√¨nh ti·∫øp t·ª•c n·ªôi dung
	planning_prompt = build_input_text(chat_history, list_of_tools_info)
	text = ""
	while True:
		output = text_completion(planning_prompt + text, stop_words=["Observation:", "Observation:\n"])
		action, action_input, output = parse_latest_tool_call(output)
		if action: # C·∫ßn ph·∫£i g·ªçi tools
			# action v√† action_input l·∫ßn l∆∞·ª£t l√† m√£ c·ªßa tool c·∫ßn g·ªçi v√† tham s·ªë ƒë·∫ßu v√†o
			# observation l√† k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ tool, d∆∞·ªõi d·∫°ng chu·ªói
			observation = call_tool(action, action_input)
			output += f"\nObservation: {observation}\nThought:"
			text += output
		else:  # Qu√° tr√¨nh sinh n·ªôi dung k·∫øt th√∫c v√† kh√¥ng c·∫ßn g·ªçi plugin n·ªØa
			text += output
			break
	new_history = []
	new_history.extend(history)
	new_history.append({'user': prompt, 'bot': text})
	return text, new_history



def build_input_text(chat_history, list_of_tools_info) -> str:
	"""T·ªïng h·ª£p l·ªãch s·ª≠ h·ªôi tho·∫°i v√† th√¥ng tin plugin th√†nh m·ªôt vƒÉn b·∫£n ƒë·∫ßu v√†o (context history)."""
	tools_text = []
	for tool_info in list_of_tools_info:
		tool = TOOL_DESC.format(
			name_for_model=tool_info["name_for_model"],
			name_for_human=tool_info["name_for_human"],
			description_for_model=tool_info["description_for_model"],
			parameters=json.dumps(tool_info["parameters"], ensure_ascii=False)
		)
		if tool_info.get("args_format", "json") == "json":
			tool += " Format the arguments as a JSON object."
		elif tool_info["args_format"] == "code":
			tool += " Enclose the code within triple backticks (`) at the beginning and end of the code."
		else:
			raise NotImplementedError
		tools_text.append(tool)
	tools_text = "\n\n".join(tools_text)

	# Tool name 
	tools_name_text = ", ".join([plugin_info["name_for_model"] for plugin_info in list_of_tools_info])

	im_start = "<|im_start|>"
	im_end = "<|im_end|>"
	prompt = f"{im_start}system\nYou are a helpful assistant.{im_end}"
	for i, (query, response) in enumerate(chat_history):
		if list_of_tools_info:  # N·∫øu c√≥ g·ªçi tool
			# Quy·∫øt ƒë·ªãnh ƒëi·ªÅn th√¥ng tin chi ti·∫øt c·ªßa tool v√†o cu·ªëi h·ªôi tho·∫°i ho·∫∑c tr∆∞·ªõc cu·ªëi h·ªôi tho·∫°i.
			if (len(chat_history) == 1) or (i == len(chat_history) - 2):
				query = PROMPT_REACT.format(
					tools_text=tools_text,
					tools_name_text=tools_name_text,
					query=query
				)
		query = query.lstrip("\n").rstrip() # Quan tr·ªçng! N·∫øu kh√¥ng √°p d·ª•ng strip, c·∫•u tr√∫c d·ªØ li·ªáu s·∫Ω kh√°c so v·ªõi c√°ch ƒë∆∞·ª£c x√¢y d·ª±ng trong qu√° tr√¨nh hu·∫•n luy·ªán.
		response = response.lstrip("\n").rstrip() # Quan tr·ªçng! N·∫øu kh√¥ng √°p d·ª•ng strip, c·∫•u tr√∫c d·ªØ li·ªáu s·∫Ω kh√°c so v·ªõi c√°ch ƒë∆∞·ª£c x√¢y d·ª±ng trong qu√° tr√¨nh hu·∫•n luy·ªán.
		# Khi s·ª≠ d·ª•ng ch·∫ø ƒë·ªô ho√†n th√†nh vƒÉn b·∫£n, b·∫°n c·∫ßn s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng sau ƒë·ªÉ ph√¢n bi·ªát gi·ªØa ng∆∞·ªùi d√πng v√† AI:
		prompt += f"\n{im_start}user\n{query}{im_end}"
		prompt += f"\n{im_start}assistant\n{response}{im_end}"

	assert prompt.endswith(f"\n{im_start}assistant\n{im_end}")
	prompt = prompt[: -len(f"{im_end}")]
	return prompt



def text_completion(input_text: str, stop_words) -> str:  # S·ª≠ d·ª•ng cho task text completion
	model = MODEL
	tokenizer = TOKENIZER
	im_end = "<|im_end|>"
	if im_end not in stop_words:
		stop_words = stop_words + [im_end]
	res = model.invoke(prompt=input_text, stop=stop_words, max_tokens=4096, temperature=0.1)
	# X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ: n·∫øu k·∫øt qu·∫£ bao g·ªìm c·∫£ input_text ban ƒë·∫ßu, lo·∫°i b·ªè n√≥ ƒëi.
	if res.startswith(input_text):
		res = res[len(input_text):]
	# Lo·∫°i b·ªè c√°c token ƒë·∫∑c bi·ªát n·∫øu c√≥
	res = res.replace("<|endoftext|>", "").replace(im_end, "")
	# C·∫Øt k·∫øt qu·∫£ n·∫øu g·∫∑p t·ª´ d·ª´ng n√†o trong stop_words
	for stop_str in stop_words:
		idx = res.find(stop_str)
		if idx != -1:
			output = res[:idx + len(stop_str)]
	return output # Tr·∫£ v·ªÅ ph·∫ßn ti·∫øp n·ªëi c·ªßa input_text


# def text_completion(input_text: str, stop_words) -> str:  # S·ª≠ d·ª•ng cho task text completion
# 	model = MODEL
# 	tokenizer = TOKENIZER
# 	im_end = "<|im_end|>"
# 	if im_end not in stop_words:
# 		stop_words = stop_words + [im_end]
# 	stop_words_ids = [tokenizer.encode(w) for w in stop_words]
# 	# stop_words_ids = [tokenizer.encode(word, add_special_tokens=False) for word in stop_words]
# 	# stop_words_ids = [token_id for sublist in stop_words_ids for token_id in sublist]
# 	stopping_criteria = StoppingCriteriaList([SequenceStoppingCriteria(stop_words_ids)])

# 	# TODO: Add sample implementation of streaming output
# 	input_ids = torch.tensor([tokenizer.encode(input_text)]).to(model.device)
# 	output = model.llm.generate(input_ids, stopping_criteria=stopping_criteria,max_length=4096,do_sample=False)
# 	output = output.tolist()[0]
# 	output = tokenizer.decode(output, errors="ignore")
# 	assert output.startswith(input_text)
# 	output = output[len(input_text):].replace('<|endoftext|>', '').replace(im_end, '')

# 	for stop_str in stop_words:
# 		idx = output.find(stop_str)
# 		if idx != -1:
# 			output = output[: idx + len(stop_str)]
# 	return output  # Ti·∫øp t·ª•c ghi k·∫øt qu·∫£ c·ªßa input_text, lo·∫°i tr·ª´ n·ªôi dung c·ªßa input_text



def parse_latest_tool_call(text):
	tool_name, tool_args = "", ""
	i = text.rfind("\nAction:")
	j = text.rfind("\nAction Input:")
	k = text.rfind("\nObservation:")
	if 0 <= i < j:  # If the text has `Action` and `Action input`,
		if k < j:  # but does not contain `Observation`,
			# then it is likely that `Observation` is skipped by the LLM,
			# because the output text may have discarded the stop word.
			text = text.rstrip() + "\nObservation:"  # Add it back.
		k = text.rfind("\nObservation:")
		tool_name = text[i + len("\nAction:") : j].strip()
		tool_args = text[j + len("\nAction Input:") : k].strip()
		text = text[:k]
	return tool_name, tool_args, text



#
# Input:
#   tool_name: Tool ƒë∆∞·ª£c g·ªçi, t∆∞∆°ng ·ª©ng v·ªõi name_for_model.
#   tool_argsÔºöTham s·ªë ƒë·∫ßu v√†o c·ªßa tool, l√† m·ªôt dict. key v√† value c·ªßa dict l·∫ßn l∆∞·ª£t l√† t√™n tham s·ªë v√† gi√° tr·ªã tham s·ªë
# Output:
#   K·∫øt qu·∫£ tr·∫£ v·ªÅ c·ªßa tool l√† d·∫°ng chu·ªói.
#   Khi ƒë·∫ßu ra ban ƒë·∫ßu l√† JSON, s·ª≠ d·ª•ng json.dumps(..., ensure_ascii=False) ƒë·ªÉ chuy·ªÉn ƒë·ªïi th√†nh chu·ªói.
#



def call_tool(tool_name: str, tool_args: str) -> str:
	img_save_path = "./"
	tokenizer = TOKENIZER
	model = MODEL
	if tool_name == "image_gen_prompt":
		try:
			img_path = json5.loads(tool_args)["image_path"]
			if img_path.startswith("http"):
				headers = {
"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
"Accept-Language": "en-US,en;q=0.5",
"Accept-Encoding": "gzip, deflate, br",
"Connection": "keep-alive",
"Upgrade-Insecure-Requests": "1"
				}
				yzmdata = requests.get(img_path, headers=headers)
				tmp_img = BytesIO(yzmdata.content)
				img = Image.open(tmp_img).convert('RGB')
				img.save(img_save_path)
				img = Image.open(img_save_path).convert('RGB')
			else:
				img = Image.open(img_path).convert('RGB')
		except:
			img_path = input(">>> Vui l√≤ng nh·∫≠p ƒë·ªãa ch·ªâ h√¨nh ·∫£nh ho·∫∑c URL: ")
			if img_path.startswith('http'):
				headers = {
"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
"Accept-Language": "en-US,en;q=0.5",
"Accept-Encoding": "gzip, deflate, br",
"Connection": "keep-alive",
"Upgrade-Insecure-Requests": "1"
				}
				yzmdata = requests.get(img_path,headers=headers)
				tmp_img = BytesIO(yzmdata.content)
				img = Image.open(tmp_img).convert('RGB')
				img.save(img_save_path)
				img = Image.open(img_save_path).convert('RGB')
			else:
				img = Image.open(img_path).convert('RGB')
		question = "Please describe all the details in this picture in detail?"
		msgs = [{"role": "user", "content": question}]
		res = model.chat(image=img, msgs=msgs, tokenizer=tokenizer)
		return res
	elif tool_name == "image_gen":
		import urllib.parse
		prompt = json5.loads(tool_args)["prompt"]
		prompt = urllib.parse.quote(prompt)
		return json.dumps({"image_url": f"https://image.pollinations.ai/prompt/{prompt}"}, ensure_ascii=False)
	elif tool_name == "modify_text":
		import urllib.parse
		prompt_input = json5.loads(tool_args)["describe_before"]
		modification_request = json5.loads(tool_args)["modification_request"]
		input_prompt = "Please modify the prompt: {}. According to the following requirements:{}. The modified prompt is: ".format(prompt_input, modification_request)
		im_start = "<|im_start|>"
		im_end = "<|im_end|>"
		prompt = f"{im_start}system\nYou are a helpful assistant.{im_end}"+f"\n{im_start}user\n{input_prompt}{im_end}"
		input_ids = torch.tensor([tokenizer.encode(prompt)]).to(model.device)
		output = model.llm.generate(input_ids, max_length=4096)
		output = output.tolist()[0]
		output = tokenizer.decode(output, errors="ignore")
		return output
	else:
		raise NotImplementedError



class SequenceStoppingCriteria(StoppingCriteria):
	"""T√πy ch·ªânh ƒëi·ªÅu ki·ªán d·ª´ng sinh chu·ªói cho LLM."""
	def __init__(self, sequence_ids):
		self.sequence_ids = sequence_ids
		self.current_sequence = []
	def check_sequences(self, current_tokens, sequences):
		"""
		Ki·ªÉm tra c√°c tokens ƒë∆∞·ª£c t·∫°o c√≥ ch·ª©a m·ªôt chu·ªói k√Ω t·ª± l·∫∑p hay kh√¥ng.

		:param current_tokens: 
			Danh s√°ch c√°c tokens hi·ªán ƒëang ƒë∆∞·ª£c t·∫°o.
		:param sequences: 
			M·ªôt danh s√°ch ch·ª©a nhi·ªÅu chu·ªói k√Ω t·ª± l·∫∑p.
		:return: 
			Tr·∫£ v·ªÅ True n·∫øu chu·ªói k√Ω t·ª± l·∫∑p n√†o xu·∫•t hi·ªán trong current_token, n·∫øu kh√¥ng th√¨ tr·∫£ v·ªÅ False.
		"""
		for i in range(len(current_tokens) - max(map(len, sequences)) + 1):
			for seq in sequences:
				if current_tokens[i:i+len(seq)] == seq:
					return True
		return False
	def __call__(self, input_ids, scores, **kwargs):
		# Nh·∫≠n c√°c tokens hi·ªán t·∫°i ƒëang ƒë∆∞·ª£c t·∫°o.
		current_tokens = [input_ids[-1][-1]]
		# Ki·ªÉm tra c√°c tokens li√™n ti·∫øp c√≥ kh·ªõp v·ªõi chu·ªói d·ª´ng kh√¥ng
		self.current_sequence.extend(current_tokens)
		# Ki·ªÉm tra xem c√°c m√£ th√¥ng b√°o hi·ªán ƒë∆∞·ª£c t·∫°o c√≥ ch·ª©a m·ªôt chu·ªói s·ªë li√™n ti·∫øp c·ª• th·ªÉ hay kh√¥ng
		if self.check_sequences(self.current_sequence, self.sequence_ids):
			return True  # D·ª´ng t·∫°o
		return False



def token_counter(messages):
	"""ƒê·∫øm s·ªë l∆∞·ª£ng token t·ª´ danh s√°ch tin nh·∫Øn."""
	tokenizer = TOKENIZER
	text = " ".join([msg.content for msg in messages])
	return len(tokenizer.encode(text)) 



def main():
	tools = [
		# {
		# 	"name_for_human": "Generate Image from Sample Image",
		# 	"name_for_model": "image_gen",
		# 	"description_for_model": "Creates a new image based on a sample image.",
		# 	"parameters": [
		# 		{
		# 			"name": "sample_image_path",
		# 			"description": "Path to the sample image. Generates variations of the sample image for data augmentation.",
		# 			"required": True,
		# 			"schema": {"type": "string"}
		# 		}
		# 	]
		# },
		{
			"name_for_human": "wenshengtu",
			"name_for_model": "image_gen_prompt",
			"description_for_model": "wenshengtu is a service that generates textual descriptions from images. By providing the URL of an image, it returns a detailed and realistic description of the image.",
			"parameters": [
				{
					"name": "image_path",
					"description": "the URL of the image to be described",
					"required": True,
					"schema": {"type": "string"},
				}
			],
		},
		{
			"name_for_human": "wenshengtu",
			"name_for_model": "image_gen",
			"description_for_model": "wenshengtu is an AI image generation service. It takes a text description as input and returns a URL of the generated image.",
			"parameters": [
				{
					"name": "prompt",
					"description": "english keywords or a text prompt describing what you want in the image.",
					"required": True,
					"schema": {"type": "string"}
				}
			]
		},
		{
			"name_for_human": "modify text",
			"name_for_model": "modify_text",
			"description_for_model": "modify Text changes the original prompt based on the input request to make it more suitable.",
			"parameters": [
				{
					"name": "describe_before",
					"description": "the prompt or image description before modification.",
					"required": True,
					"schema": {"type": "string"}
				},
				{
					"name": "modification_request",
					"description": "the request to modify the prompt or image description, e.g., change 'cat' to 'dog' in the text.",
					"required": True,
					"schema": {"type": "string"}
				}
			]
		}
	]
	history = []
	for query in ["Hello", "Who is Jay Chou", "Who is his wife", "Draw me a cute kitten, preferably a black cat", "exit"]:
		if query.lower() == "exit":
			break 
		response, history = llm_with_tools(prompt=query, history=history, list_of_tools_info=tools)
		print(f">>>ü§ñDeepseek-r1 response:\n{response}\n")



if __name__ == "__main__":
	fire.Fire(main) 










































































# def select_tools(state: State) -> State:
# 	query = state["messages"][-1].content
# 	tool_docs = tools_retriever.invoke(query)
# 	return {"selected_tools": [doc.metadata["name"] for doc in tool_docs]}

# def reflect(state: State) -> State:
# 	class_map = {
# 		AIMessage: HumanMessage, 
# 		HumanMessage: AIMessage, 
# 		ToolMessage: HumanMessage 
# 	}
# 	translated = [reflection_prompt, state["messages"][0]] + [
# 		class_map[msg.__class__](content=msg.content) 
# 		for msg in state["messages"][1:]
# 	]
# 	answer = model.invoke(translated)
# 	return {"messages": [HumanMessage(content=answer.content)]}

# def should_continue(state: State):
# 	if len(state["messages"]) > 6:
# 		return END
# 	else:
# 		return "reflect"

# def chatbot(state: State) -> State:
# 	selected_tools = [tool for tool in tools if tool.name in state["selected_tools"]]
# 	answer = model.bind_tools(selected_tools).invoke([generate_prompt] + state["messages"])
# 	return {"messages": [answer]}

# def main():
# 	"""Th·ª±c thi ch∆∞∆°ng tr√¨nh."""
# 	builder = StateGraph(State)

# 	builder.add_node("select_tools", select_tools)
# 	builder.add_node("chatbot", chatbot)
# 	builder.add_node("tools", ToolNode(tools))
# 	builder.add_node("reflect", reflect)

# 	builder.add_edge(START, "select_tools")
# 	builder.add_edge("select_tools", "chatbot")
# 	builder.add_conditional_edges("chatbot", tools_condition)
# 	builder.add_edge("tools", "chatbot")
# 	builder.add_conditional_edges("chatbot", should_continue)
# 	builder.add_edge("reflect", "chatbot")
	
# 	graph = builder.compile(checkpointer=MemorySaver())

# 	user_input = {
# 		"messages": [HumanMessage("""What is Large Language Model?""")]
# 	}
# 	for chunk in graph.stream(user_input, config):
# 		print(chunk)

# if __name__ == "__main__":
# 	fire.Fire(main)
