# T√†i li·ªáu li√™n quan:  
# 	Gi·ªõi thi·ªáu ng·∫Øn g·ªçn v·ªÅ nguy√™n l√Ω ReAct Prompting, kh√¥ng bao g·ªìm m√£ ngu·ªìn:  
#		https://github.com/QwenLM/Qwen-7B/blob/main/examples/react_prompt.md  
#   Tri·ªÉn khai ReAct Prompting d·ª±a tr√™n giao di·ªán `model.chat` 
# 			(ch·∫ø ƒë·ªô ƒë·ªëi tho·∫°i), bao g·ªìm c·∫£ t√≠ch h·ª£p c√¥ng c·ª• v·ªõi LangChain:  
#		https://github.com/QwenLM/Qwen-7B/blob/main/examples/langchain_tooluse.ipynb  
#   Tri·ªÉn khai ReAct Prompting d·ª±a tr√™n giao di·ªán `model.generate` 
# 			(ch·∫ø ƒë·ªô ti·∫øp t·ª•c vi·∫øt), ph·ª©c t·∫°p h∆°n so v·ªõi ch·∫ø ƒë·ªô chat:  
#       https://github.com/QwenLM/Qwen-7B/blob/main/examples/react_demo.py (t·ªáp n√†y)  
# 	T√†i li·ªáu s·ª≠ d·ª•ng ChatOllama l√†m agent_core:
# 		 https://python.langchain.com/docs/integrations/providers/ollama/



from datetime import datetime 
import tqdm
import requests
import json
import json5
import fire 
from PIL import Image
from io import BytesIO



from pydantic import BaseModel, Field



from langchain.tools import StructuredTool 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms.fake import FakeStreamingListLLM
from langchain_core.messages import (AIMessage, HumanMessage, ToolMessage)



# Constant vars 
TOOLS = [
	{
		"name_for_human": "image_to_text", 
		"name_for_model": "image_to_text", 
		"description_for_model": "image_to_text is a service that generates textual descriptions from images. By providing the URL of an image, it returns a detailed and realistic description of the image.",
		"parameters": [
			{
				"name": "image_path",
				"description": "the URL of the image to be described",
				"required": True,
				"schema": {"type": "string"},
			}
		],
	},
	{
		"name_for_human": "text_to_image",
		"name_for_model": "text_to_image",
		"description_for_model": "text_to_image is an AI image generation service. It takes a text description as input and returns a URL of the generated image.",
		"parameters": [
			{
				"name": "text",
				"description": "english keywords or a text prompt describing what you want in the image.",
				"required": True,
				"schema": {"type": "string"}
			}
		]
	},
	{
		"name_for_human": "modify_text",
		"name_for_model": "modify_text",
		"description_for_model": "modify_text changes the original prompt based on the input request to make it more suitable.",
		"parameters": [
			{
				"name": "describe_before",
				"description": "the prompt or image description before modification.",
				"required": True,
				"schema": {"type": "string"}
			},
			{
				"name": "modification_request",
				"description": "the request to modify the prompt or image description, e.g., change 'cat' to 'dog' in the text.",
				"required": True,
				"schema": {"type": "string"}
			}
		]
	}, 
	{
		"name_for_human": "ai_vision", 
		"name_for_model": "ai_vision", 
		"description_for_model": "ai_vision is a service that detects and extracts characters from a product image. It processes the image and returns the recognized text to the AI agent for further analysis.",
		"parameters": [
			{
				"name": "video_path",
				"description": "The file path of a video or the IP address of a camera for real-time character detection.",
				"required": True,
				"schema": {"type": "string"}
			}
		],
	},
]



FAKE_RESPONSES = [
# "Hello, Good afternoon!", -- Fake resp id 0 -- No tool
	"""
	Thought: The input is a greeting. No tools are needed.
	Final Answer: Hello! Good afternoon! How can I assist you today?
	""", 
# "Who is Jay Chou?", -- Fake resp id 1 -- No tool 
	"""
	Thought: The user is asking for information about Jay Chou. I should retrieve general knowledge.
	Final Answer: Jay Chou is a Taiwanese singer, songwriter, and actor, widely known for his influence in Mandopop music. He has released numerous albums and is recognized for his unique blend of classical and contemporary music.
	""", 
# "Who is his wife?", -- Fake resp id 2 -- No tool 
	"""
	Thought: The previous question was about Jay Chou. "His wife" likely refers to Jay Chou's spouse.
	Final Answer: Jay Chou's wife is Hannah Quinlivan, an actress and model from Taiwan.
	""", 
# "Describe what is in this image, this is URL of the image: https://www.night_city_img.com", --> Fake resp id 3 -- Tool-use: image_to_text
	"""
	Thought: The user wants a description of an image. I should use the image_to_text API.
	Action: image_to_text
	Action Input: {"image_path": "https://www.tushengwen.com"}
	Observation: "The image depicts a vibrant cityscape at night, illuminated by neon lights and tall skyscrapers."
	Thought: I now know the final answer.
	Final Answer: The image depicts a vibrant cityscape at night, illuminated by neon lights and tall skyscrapers.
	""",
# "Draw me a cute kitten, preferably a black cat", -- Fake resp id 4 -- Tool-use: text_to_image
	"""
	Thought: The user is requesting an image generation. I should use the text_to_image API.
	Action: text_to_image
	Action Input: {"text": "A cute black kitten with big eyes, fluffy fur, and a playful expression"}
	Observation: Here is the generated image URL: [https://www.wenshengtu.com]
	Thought: I now know the final answer.
	Final Answer: Here is an image of a cute black kitten: [https://www.wenshengtu.com]
	""", 
# "Modify this description: 'A blue Honda car parked on the street' to 'A red Mazda car parked on the street'", -- Fake resp id 5 -- Tool-use: modify_text
	"""
	Thought: The user wants to modify a text description. They want change from `A blue honda car parked on the street` to `A red Mazda car parked on the street`.
	Final Answer: "A red Mazda car parked on the street."
	""", 
# Check ch·ªØ tr√™n s·∫£n ph·∫©m
	"""
	T·∫°m Th·ªùi:
	Nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu x·ª≠ l√Ω t·ª´ AI-Vision, bao g·ªìm 1. fuzetea, Fuzetea l√† m·ªôt nh√£n hi·ªáu tr√† gi·∫£i kh√°t ƒë∆∞·ª£c ph√¢n ph·ªëi t·∫°i Vi·ªát nam, c√°c th√¥ng tin chu·ªói nh·∫≠n di·ªán ƒë∆∞·ª£c c√≤n l·∫°i nh∆∞ passion fruit tea and chia seeds and youthful-life-every day ƒë·ªÅu ƒë·∫ßy ƒë·ªß ng·ªØ nghƒ©a -- s·∫£n ph·∫©m OK.
	"""
# "exit",
	"""Goodbye! Have a great day! üòä"""
]



DICT_FAKE_RESPONSES = {idx: fresp for idx, fresp in enumerate(FAKE_RESPONSES)}



MODEL = FakeStreamingListLLM(responses=[""])



EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")



TOOL_DESC = """{name_for_model}: Call this tool to interact with the {name_for_human} API. 
What is the {name_for_human} API useful for? 
{description_for_model}.
Parameters: {parameters}"""



PROMPT_REACT = """You are an AI assistant that follows the ReAct reasoning framework. 
You have access to the following APIs:

{tools_desc}

Use the following strict format:

### Input Format:

Question: [The input question]
Thought: [Think logically about the next step]
Action: [Select from available tools: {tools_name}]
Action Input: [Provide the required input]
Observation: [Record the output from the action]
... (Repeat the Thought/Action/Observation loop as needed)
Thought: I now know the final answer
Final Answer: [Provide the final answer]

Begin!

Question: {query}"""



STOP_WORDS = ["Observation:", "Observation:\n"]



class ResponseWithChainOfThought(BaseModel):
	"""LLM xu·∫•t output ƒë·ªãnh d·∫°ng ReAct khi g·∫∑p truy v·∫•n c·∫ßn CoT."""
	question: str
	thought: str
	action: str
	action_input: str
	observation: str
	final_thought: str
	final_answer: str



# H√†m ƒë·∫ßu v√†o ch√≠nh c·ªßa ƒëo·∫°n m√£ v√≠ d·ª• n√†y.
#
# Input:
# prompt: query m·ªõi nh·∫•t t·ª´ ‚Äã‚Äãng∆∞·ªùi d√πng.
#   history: L·ªãch s·ª≠ h·ªôi tho·∫°i gi·ªØa ng∆∞·ªùi d√πng v√† m√¥ h√¨nh, d∆∞·ªõi d·∫°ng m·ªôt list.
#       m·ªói ph·∫ßn t·ª≠ trong danh s√°ch c√≥ d·∫°ng:
#           {"user": "query c·ªßa user", "agent": "respond c·ªßa agent"}.
#       h·ªôi tho·∫°i m·ªõi nh·∫•t s·∫Ω n·∫±m ·ªü cu·ªëi danh s√°ch. Kh√¥ng bao g·ªìm c√¢u h·ªèi m·ªõi nh·∫•t. 
#   tools: Danh s√°ch c√°c tools c√≥ th·ªÉ s·ª≠ d·ª•ng, ƒë∆∞·ª£c l∆∞u trong m·ªôt list.
#       v√≠ d·ª• tools = [tool_info_0, tool_info_1, tool_info_2]Ôºå
#       trong ƒë√≥ tool_info_0, tool_info_1, tool_info_2 l√† th√¥ng tin chi ti·∫øt c·ªßa 
#           t·ª´ng plugin, ƒë√£ ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥ trong t√†i li·ªáu n√†y.
#
# output:
#   ph·∫£n h·ªìi c·ªßa agent cho query c·ªßa ng∆∞·ªùi d√πng. 



def llm_with_tools(query, history, tools, idx):
	# TODO: L·ªãch s·ª≠ h·ªôi tho·∫°i c√†ng to, th·ªùi gian th·ª±c thi v√≤ng l·∫∑p for c√†ng l·ªõn. L√†m sao ƒë·ªÉ gi·∫£i quy·∫øt?
	chat_history = [(x["user"], x["bot"]) for x in history] + [(query, "")]
	# Ng·ªØ c·∫£nh tr√≤ chuy·ªán ƒë·ªÉ m√¥ h√¨nh ti·∫øp t·ª•c n·ªôi dung
	planning_prompt = build_input_text(chat_history=chat_history, tools=tools)
	text = ""
	while True:
		resp = model_invoke(input_text=planning_prompt+text, idx=idx)
		action, action_input, output = parse_latest_tool_call(resp=resp) 
		if action: # C·∫ßn ph·∫£i g·ªçi tools 
			# action v√† action_input l·∫ßn l∆∞·ª£t l√† tool c·∫ßn g·ªçi v√† tham s·ªë ƒë·∫ßu v√†o
			# observation l√† k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ tool, d∆∞·ªõi d·∫°ng chu·ªói
			res = tool_exe(
				tool_name=action, tool_args=action_input, idx=idx
			)
			text += res
			break
		else:  # Qu√° tr√¨nh sinh n·ªôi dung k·∫øt th√∫c v√† kh√¥ng c·∫ßn g·ªçi tool 
			text += output
			break
	new_history = []
	new_history.extend(history)
	new_history.append(
		{'user': query, 'bot': text}
	)
	idx += 1
	return text, new_history



def build_input_text(
		chat_history, 
		tools, 
		im_start = "<|im_start|>", 
		im_end = "<|im_end|>"
	):
	"""T·ªïng h·ª£p l·ªãch s·ª≠ h·ªôi tho·∫°i v√† th√¥ng tin plugin th√†nh m·ªôt vƒÉn b·∫£n ƒë·∫ßu v√†o (context history)."""
	prompt = f"{im_start}system\nYou are a helpful assistant.{im_end}"
	tools_text = []
	for tool_info in tools:
		tool = TOOL_DESC.format(
			name_for_model=tool_info["name_for_model"],
			name_for_human=tool_info["name_for_human"],
			description_for_model=tool_info["description_for_model"],
			parameters=json.dumps(tool_info["parameters"], ensure_ascii=False)
		)
		if dict(tool_info).get("args_format", "json") == "json":
			tool += ". Format the arguments as a JSON object."
		elif dict(tool_info).get("args_format", "code") == "code":
			tool += ". Enclose the code within triple backticks (`) at the beginning and end of the code."
		else:
			raise NotImplementedError
		tools_text.append(tool)
	tools_desc = "\n\n".join(tools_text)
	tools_name = ", ".join([tool_info["name_for_model"] for tool_info in tools])
	for i, (query, response) in enumerate(chat_history):
		# TODO: Nghi√™n c·ª©u th√™m RAG-tools-call x·ª≠ l√Ω g·ªçi tool 
		if tools:  # N·∫øu c√≥ g·ªçi tool 
			# Quy·∫øt ƒë·ªãnh ƒëi·ªÅn th√¥ng tin chi ti·∫øt c·ªßa tool v√†o cu·ªëi h·ªôi tho·∫°i ho·∫∑c tr∆∞·ªõc cu·ªëi h·ªôi tho·∫°i.
			# TODO: C·∫ßn l√†m r√µ d√≤ng l·ªánh if -- t·∫°i line 244
			if (len(chat_history) == 1) or (i == len(chat_history) - 2):
				query = PROMPT_REACT.format(
					tools_desc=tools_desc, 
					tools_name=tools_name, 
					query=query
				)
		query = query.strip() # Quan tr·ªçng! N·∫øu kh√¥ng √°p d·ª•ng strip, c·∫•u tr√∫c d·ªØ li·ªáu s·∫Ω kh√°c so v·ªõi c√°ch ƒë∆∞·ª£c x√¢y d·ª±ng trong qu√° tr√¨nh hu·∫•n luy·ªán.
		if isinstance(response, str):
			response = response.strip()
		elif not response:
			raise ValueError(">>> Error: response is None or empty, expected a string.")  
		else:
			try:
				response = str(response).strip() # Quan tr·ªçng! N·∫øu kh√¥ng √°p d·ª•ng strip, c·∫•u tr√∫c d·ªØ li·ªáu s·∫Ω kh√°c so v·ªõi c√°ch ƒë∆∞·ª£c x√¢y d·ª±ng trong qu√° tr√¨nh hu·∫•n luy·ªán.
			except Exception as e:
				raise e
		# Trong text_completion, s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng sau ƒë·ªÉ ph√¢n bi·ªát gi·ªØa User v√† AI 
		prompt += f"\n{im_start}user\n{query}{im_end}"
		prompt += f"\n{im_start}assistant\n{response}{im_end}"
	assert prompt.endswith(f"\n{im_start}assistant\n{im_end}")
	prompt = prompt[: -len(f"{im_end}")]
	return prompt



def model_invoke(input_text, idx):
	"""Text completion, sau ƒë√≥ ch·ªânh s·ª≠a k·∫øt qu·∫£ inference output."""
	res = MODEL.invoke(input=input_text)
	res = llm_fake_response(idx=idx)
	return res 



def llm_fake_response(idx):
	"""Gi·∫£ l·∫≠p k·∫øt qu·∫£ inference LLM."""
	return DICT_FAKE_RESPONSES[idx]



def parse_latest_tool_call(resp):
	"""X·ª≠ l√Ω k·∫øt qu·∫£ inference LLM, ph√¢n t√≠ch chu·ªói ƒë·ªÉ th·ª±c thi c√¥ng c·ª•."""
	tool_name, tool_args = "", ""
	action = str(resp).rfind("Action:")
	action_input = str(resp).rfind("Action Input:")
	observation = str(resp).rfind("Observation:")
	if 0 <= action < action_input < observation:
		tool_name = str(resp[action + len("Action:") : action_input]).strip()
		tool_args = str(resp[action_input + len("Action Input:") : observation]).strip()
		resp = resp[:observation]
	return tool_name, tool_args, resp



def text_to_image(tool_args):
	import urllib.parse
	prompt = json5.loads(tool_args)["text"]
	prompt = urllib.parse.quote(prompt)
	return json.dumps({"image_url": f"https://image.pollinations.ai/prompt/{prompt}"}, ensure_ascii=False)



def image_to_text(tool_args, idx, img_save_path="./"):
	# Gi·∫£ l·∫≠p th·ª±c thi c√¥ng c·ª• image_to_text
	if "": 
		img = request_image_from_web(
			tool_args=tool_args, 
			img_save_path=img_save_path
		)
	resp = llm_fake_response(idx=idx)
	return resp[resp.rfind("Final Answer") :]



def llm_vision(tool_args, idx):
	import numpy as np 
	import cv2 
	from paddleocr import PaddleOCR, draw_ocr
	from PIL import Image 
	vid_path = "G:/tranvantuan/fuzetea_vid2.mp4"
	ocr = PaddleOCR(use_angle_cls=True, lang='en') 
	cap = cv2.VideoCapture(vid_path)
	if not cap.isOpened():
		print(">>> Can not open camera")
		exit()
	print(">>> Starting real-time OCR. Press 'q' to exit.")
	while True:
		ret, frame = cap.read()
		if not ret:
			print(">>> Can't receive frame (stream end?). Exiting...")
			break 
		# Perform OCR on the current frame
		result = ocr.ocr(frame, cls=False)
		# Draw detected text on the frame
		for res in result:
			for line in res:
				box, (text, score) = line 
				box = np.array(box, dtype=np.int32)
				# Draw bounding box
				cv2.polylines(frame, [box], isClosed=True, color=(0, 255, 0), thickness=2)
				# Display text near the bounding box
				x, y = box[0]
				cv2.putText(frame, f"{text} ({score:.2f})", (x, y - 10), 
				cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

		cv2.imshow("Research Demo AI-Agent create AI-Vision", frame)
		if cv2.waitKey(1) & 0xFF == ord('q'):
			break 

	cap.release()
	cv2.destroyAllWindows()
	print(">>> OCR session ended.")
# Input:
#   tool_name: Tool ƒë∆∞·ª£c g·ªçi, t∆∞∆°ng ·ª©ng v·ªõi name_for_model.
#   tool_argsÔºöTham s·ªë ƒë·∫ßu v√†o c·ªßa tool, l√† m·ªôt dict. key v√† value c·ªßa dict l·∫ßn l∆∞·ª£t l√† t√™n tham s·ªë v√† gi√° tr·ªã tham s·ªë
# Output:
#   K·∫øt qu·∫£ tr·∫£ v·ªÅ c·ªßa tool l√† d·∫°ng chu·ªói.
#   Khi ƒë·∫ßu ra ban ƒë·∫ßu l√† JSON, s·ª≠ d·ª•ng json.dumps(..., ensure_ascii=False) ƒë·ªÉ chuy·ªÉn ƒë·ªïi th√†nh chu·ªói.



def tool_exe(tool_name: str, tool_args: str, idx: int, video_path: str) -> str:
	"""Th·ª±c thi c√¥ng c·ª• (tool execution) ƒë∆∞·ª£c LLM g·ªçi."""
	if tool_name == "image_to_text":
		resp = image_to_text(
			tool_args=tool_args, idx=idx
		)
		return resp
	elif tool_name == "text_to_image":
		resp = text_to_image(tool_args=tool_args)
	elif tool_name == "llm_vision":
		resp = llm_vision(
			tool_args=tool_args, 
			idx=idx, 
		)
		return resp
	else:
		raise NotImplementedError



def request_image_from_web(tool_args, img_save_path="./"):
	try:
		img_path = json5.loads(tool_args)["image_path"]
		if str(img_path).startswith("http"):
			headers = {
				"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
				"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
				"Accept-Language": "en-US,en;q=0.5",
				"Accept-Encoding": "gzip, deflate, br",
				"Connection": "keep-alive",
				"Upgrade-Insecure-Requests": "1"
			}
			yzmdata = requests.get(url=img_path, headers=headers)
			tmp_img = BytesIO(yzmdata.content)
			img = Image.open(tmp_img).convert('RGB')
			img.save(img_save_path)
			img = Image.open(img_save_path).convert('RGB')
			return img
		else:
			img = Image.open(img_path).convert('RGB')
			return img 
	except:
		img_path = input(">>> Vui l√≤ng nh·∫≠p ƒë·ªãa ch·ªâ h√¨nh ·∫£nh ho·∫∑c URL: ")
		if img_path.startswith('http'):
			headers = {
				"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
				"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
				"Accept-Language": "en-US,en;q=0.5",
				"Accept-Encoding": "gzip, deflate, br",
				"Connection": "keep-alive",
				"Upgrade-Insecure-Requests": "1"
			}
			yzmdata = requests.get(img_path,headers=headers)
			tmp_img = BytesIO(yzmdata.content)
			img = Image.open(tmp_img).convert('RGB')
			img.save(img_save_path)
			img = Image.open(img_save_path).convert('RGB')
			return img
		else:
			img = Image.open(img_path).convert('RGB')
			return img 



def main():
	history = []
	for idx, query in tqdm.tqdm(enumerate([
		"Hello, Good afternoon!", # -- id 0 
		"Who is Jay Chou?", # -- id 1
		"Who is his wife?", # -- id 2
		"Describe what is in this image, this is URL of the image: https://www.night_city_img.com", # -- id 3
		"Draw me a cute kitten, preferably a black cat", # -- id 4
		"Modify this description: 'A blue Honda car parked on the street' to 'A red Mazda car parked on the street'", # --id 5
		"I need to verify the characters on this product. Here is the image path of the product: G:/tranvantuan/fuzetea.jpg", # --id 6
		"exit" 
	])):
		print("\n")
		print(f">>> üßë query:\n\t{query}\n")
		if query.lower() == "exit":
			print(f">>> ü§ñ response:\nGoodbye! Have a great day! üòä\n")
			break 
		response, history = llm_with_tools(
			query=query, 
			history=history, 
			tools=TOOLS, 
			idx=idx
		)
		print(f">>> ü§ñ response:\n{response}\n")



if __name__ == "__main__":
	fire.Fire(main) 



# TODO: 
# K·ªãch b·∫£n Demo -- T√¥i c·∫ßn b·∫°n gi√∫p t√¥i tracking ch·ªØ tr√™n v·∫≠t th·ªÉ n√†y, 
# v·ªõi c√°c y√™u c·∫ßu sau: 
# 1. Ch·ªØ c√≥ ƒë·ªß kh√¥ng. 
# 2. C√≥ b·ªã sai nghƒ©a kh√¥ng


if False:
		
	import numpy as np 
	import cv2 
	from paddleocr import PaddleOCR, draw_ocr
	from PIL import Image


	vid_path = "G:/tranvantuan/fuzetea_vid2.mp4"



	ocr = PaddleOCR(use_angle_cls=True, lang='en') 



	cap = cv2.VideoCapture(vid_path)
	if not cap.isOpened():
		print(">>> Can not open camera")
		exit()
	print(">>> Starting real-time OCR. Press 'q' to exit.")
	while True:
		ret, frame = cap.read()
		if not ret:
			print(">>> Can't receive frame (stream end?). Exiting...")
			break 
		# Perform OCR on the current frame
		result = ocr.ocr(frame, cls=False)
		# Draw detected text on the frame
		for res in result:
			for line in res:
				box, (text, score) = line 
				box = np.array(box, dtype=np.int32)
				# Draw bounding box
				cv2.polylines(frame, [box], isClosed=True, color=(0, 255, 0), thickness=2)
				# Display text near the bounding box
				x, y = box[0]
				cv2.putText(frame, f"{text} ({score:.2f})", (x, y - 10), 
				cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

		cv2.imshow("Research Demo AI-Agent create AI-Vision", frame)
		if cv2.waitKey(1) & 0xFF == ord('q'):
			break 

	cap.release()
	cv2.destroyAllWindows()
	print(">>> OCR session ended.")

	# img_path = 'G:/tranvantuan/fuzetea.jpg'
	# result = ocr.ocr(image, cls=True)
	# for idx in range(len(result)):
	#     res = result[idx]
	#     for line in res:
	#         print(line)



	# # draw result
	# result = result[0]
	# image = Image.open(img_path).convert('RGB')
	# boxes = [line[0] for line in result]
	# txts = [line[1][0] for line in result]
	# scores = [line[1][1] for line in result]
	# im_show = draw_ocr(image, boxes, txts, scores)
	# im_show = Image.fromarray(im_show)
	# im_show.save('result.jpg')
