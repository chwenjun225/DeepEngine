{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "\t\"\"\"Use this to get weather information.\"\"\"\n",
    "\tif city == \"nyc\":\n",
    "\t\treturn \"It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees\"\n",
    "\telif city == \"sf\":\n",
    "\t\treturn \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"\n",
    "\telse:\n",
    "\t\traise AssertionError(\"Unknown city\")\n",
    "\n",
    "\n",
    "\n",
    "tools = [get_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThoughtResponse(BaseModel):\n",
    "\t\"\"\"Response to the user with this structured output.\"\"\"\n",
    "\tuser_prompt: str = Field(description=\"The original prompt provided by the user.\")\n",
    "\tthought: str = Field(description=\"Logical reasoning before deciding the next step.\")\n",
    "\taction: str = Field(description=\"The action to be taken, chosen from available tools {}.\".format(\", \".join([tool.name for tool in tools])))\n",
    "\taction_input:str = Field(description=\"The required input to perform the selected action.\") \n",
    "\tobservation: str = Field(description=\"The outcome or response from executing the action.\") \n",
    "\tjustification: str = Field(description=\"The explanation of why the final answer is relevant to the original prompt provided by the user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "\tresponse: ChainOfThoughtResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "model = ChatOllama(model=\"llama3.2:1b-instruct-fp16\", temperature=0.1, num_predict=\"2048\")\n",
    "graph = create_react_agent(\n",
    "\tname=\"chatbot_react_agent\", \n",
    "\tmodel=model, \n",
    "\ttools=tools_node, \n",
    "\tresponse_format=ResponseWithChainOfThought,\n",
    "\tcheckpointer=memory, \n",
    "\tprompt=SystemMessage(content=\"You are a helpful assistant!\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "inputs = {\"messages\": [\n",
    "\tSystemMessage(\"You are helpful assistant, if the tool get error, you can tell that you don't know the answer\"), \n",
    "\tHumanMessage(\"what is the weather in sf\")\n",
    "]}\n",
    "for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "\tmessage = s[\"messages\"][-1]\n",
    "\tif isinstance(message, tuple):\n",
    "\t\tprint(message)\n",
    "\telse:\n",
    "\t\tmessage.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [HumanMessage(\"What is the capital of France?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(\"\"\"You are a helpful assistant that responds to questions with three exclamation marks.\"\"\")\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "template.invoke({\"context\": \"\"\"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\"\"\",\n",
    "\t\"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class AnswerWithJustification(BaseModel):\n",
    "\t'''An answer to the user's question along with justification for the answer.'''\n",
    "\tanswer: str\n",
    "\t'''The answer to the user's question'''\n",
    "\tjustification: str\n",
    "\t'''Justification for the answer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
    "structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
