{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def get_llm(\n",
    "\t\tport=2026, host=\"127.0.0.1\", version=\"v1\", \n",
    "\t\ttemperature=0, openai_api_key=\"chwenjun225\",\n",
    "\t\tmodel_name=\"/home/chwenjun225/Projects/Foxer/models/Llama-3.2-1B-Instruct\"\n",
    "\t):\n",
    "\t\"\"\"Khởi tạo mô hình ngôn ngữ lớn DeepSeek-R1 từ LlamaCpp-Server.\"\"\"\n",
    "\topenai_api_base = f\"\"\"http://{host}:{port}/{version}\"\"\" \n",
    "\treturn ChatOpenAI(\n",
    "\t\tmodel=model_name, \n",
    "\t\topenai_api_base=openai_api_base, \n",
    "\t\topenai_api_key=openai_api_key, \n",
    "\t\ttemperature=temperature\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_llm(\n",
    "\tport=2026, host=\"127.0.0.1\", version=\"v1\", \n",
    "\ttemperature=0, openai_api_key=\"chwenjun225\",\n",
    "\tmodel_name=\"/home/chwenjun225/Projects/Foxer/models/Llama-3.2-1B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [HumanMessage(\"What is the capital of France?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 42, 'total_tokens': 50, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': '/home/chwenjun225/Projects/Foxer/models/Llama-3.2-1B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f64c4a1a-8042-422d-bbe7-30e651bcb1a3-0', usage_metadata={'input_tokens': 42, 'output_tokens': 8, 'total_tokens': 50, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 57, 'total_tokens': 65, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': '/home/chwenjun225/Projects/Foxer/models/Llama-3.2-1B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-76f4dbff-601d-4e10-8f22-aef8d22175e9-0', usage_metadata={'input_tokens': 57, 'output_tokens': 8, 'total_tokens': 65, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_msg = SystemMessage(\"\"\"You are a helpful assistant that responds to questions with three exclamation marks.\"\"\")\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: ')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "template.invoke({\"context\": \"\"\"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\"\"\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''An answer to the user's question along with justification for the answer.'''\n",
    "    answer: str\n",
    "    '''The answer to the user's question'''\n",
    "    justification: str\n",
    "    '''Justification for the answer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fda35259dd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fda342b6910>, root_client=<openai.OpenAI object at 0x7fda34cb8410>, root_async_client=<openai.AsyncOpenAI object at 0x7fda35cde410>, model_name='/home/chwenjun225/Projects/Foxer/models/Llama-3.2-1B-Instruct', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://127.0.0.1:2026/v1'), kwargs={'response_format': <class '__main__.AnswerWithJustification'>, 'structured_output_format': {'kwargs': {'method': 'json_schema'}, 'schema': {'type': 'function', 'function': {'name': 'AnswerWithJustification', 'description': \"An answer to the user's question along with justification for the answer.\", 'parameters': {'properties': {'answer': {'type': 'string'}, 'justification': {'type': 'string'}}, 'required': ['answer', 'justification'], 'type': 'object'}}}}}, config={}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_oai_structured_outputs_parser), kwargs={}, config={}, config_factories=[], custom_output_type=<class '__main__.AnswerWithJustification'>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
    "structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foxer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
