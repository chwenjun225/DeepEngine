{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import numpy as np \n",
    "import torch \n",
    "from torch.nn import Parameter \n",
    "from torch.nn.modules.module import  Module \n",
    "import torch.nn.functional as F\n",
    "import math \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng BaseModel \n",
    "class PruningModel(Module):\n",
    "\tdef prune_by_std(self, s=0.25):\n",
    "\t\t\"\"\"\n",
    "\t\t\tCắt tỉa trọng số sử dụng độ lệch chuẩn (std - \n",
    "\t\t\tstandard deviation) được gọi là ngưỡng (threshold)\n",
    "\t\t\"\"\"\n",
    "\t\t# Lưu ý rằng thuật ngữ `module`` ở đây được hiểu là các \n",
    "\t\t# `lớp (layer)`, ví dụ: fc1, fc2, fc3\n",
    "\t\tfor name, module in self.named_modules():\n",
    "\t\t\tif name in ['fc1', 'fc2', 'fc3']:\n",
    "\t\t\t\tthreshold = np.std(module.weight.data.cpu().numpy()) * s\n",
    "\t\t\t\tprint(f\">>> Cắt tỉa với ngưỡng: {threshold} cho layer {name}\")\n",
    "\t\t\t\tmodule.prune(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng module cắt tỉa \n",
    "class MaskedLinear(Module):\n",
    "\tdef __init__(self, in_features, out_features, bias=True):\n",
    "\t\tsuper(MaskedLinear, self).__init__()\n",
    "\t\tself.in_features = in_features\n",
    "\t\tself.out_features = out_features\n",
    "\t\tself.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "\t\t# Khởi tạo bộ lọc (mask-mặt nạ) cho ta quyết định \n",
    "\t\t# weight nào được tính toán, weight nào không\n",
    "\t\tself.mask = Parameter(\n",
    "\t\t\ttorch.ones([out_features, in_features]), \n",
    "\t\t\trequires_grad=False\n",
    "\t\t)\n",
    "\t\tif bias: \n",
    "\t\t\tself.bias = Parameter(torch.Tensor(out_features))\n",
    "\t\telse:\n",
    "\t\t\tself.register_parameter('bias', None)\n",
    "\t\tself.reset_parameters() \n",
    "\n",
    "\tdef reset_parameters(self):\n",
    "\t\tstdv = 1. / math.sqrt(self.weight.size(1))\n",
    "\t\t# Phân phối đều trọng số mô hình trong khoảng [-stdv, stdv]\n",
    "\t\tself.weight.data.uniform_(-stdv, stdv)\n",
    "\t\tif self.bias is not None:\n",
    "\t\t\tself.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\t# Nhân weight với bộ lọc trước. Điều này giúp loại \n",
    "\t\t# bỏ đi các weight không cần thiết sau khi đã cắt tỉa\n",
    "\t\treturn F.linear(input, self.weight * self.mask, self.bias)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn self.__class__.__name__ + '(' \\\n",
    "\t\t\t+ 'in_features=' + str(self.in_features) \\\n",
    "\t\t\t+ ', out_features=' + str(self.out_features) \\\n",
    "\t\t\t+ ', bias=' + str(self.bias is not None) + ')'\n",
    "\n",
    "\tdef prune(self, threshold):\n",
    "\t\t\"\"\"\n",
    "\t\t\tHàm tuỳ chỉnh cắt tỉa (prune) với bộ lọc (mask). Tại mỗi \n",
    "\t\t\tlần cắt tỉa, tính toán các trong số nào có weight nhỏ \n",
    "\t\t\thơn ngưỡng quy định, cập nhật lại bộ lọc (mask) và weight \n",
    "\t\t\ttại các vị trí đó về giá trị 0.\n",
    "\t\t\"\"\"\n",
    "\t\tweight_device = self.weight.device\n",
    "\t\tmask_device = self.mask.device \n",
    "\t\t# Đưa tensor từ GPU về CPU và chuyển tensor về mảng numpy\n",
    "\t\ttensor = self.weight.data.cpu().numpy()\n",
    "\t\tmask = self.mask.data.cpu().numpy()\n",
    "\t\t# Sau khi cắt tỉa những weight nào không cần nữa thì sẽ thành số 0\n",
    "\t\tnew_mask = np.where(abs(tensor) < threshold, 0, mask)\n",
    "\t\t# Apply trọng số và bộ lọc (mask-mặt nạ) mới\n",
    "\t\tself.weight.data = torch.from_numpy(tensor * new_mask).to(weight_device)\n",
    "\t\tself.mask.data = torch.from_numpy(new_mask).to(mask_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt mạng FullyConnected, kết nối tất cả module (layer-lớp) lại với nhau\n",
    "class LeNet(PruningModel):\n",
    "\tdef __init__(self, mask=False):\n",
    "\t\tsuper(LeNet, self).__init__()\n",
    "\t\tlinear = MaskedLinear if mask else nn.Linear \n",
    "\t\tself.fc1 = linear(784, 300)\n",
    "\t\tself.fc2 = linear(300, 100)\n",
    "\t\tself.fc3 = linear(100, 10)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(-1, 784)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = F.log_softmax(self.fc3(x), dim=1)\n",
    "\t\treturn x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt hyperparameter \n",
    "# Define some const\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "USE_CUDA = True\n",
    "SEED = 42\n",
    "LOG_AFTER = 10 # How many batches to wait before logging training status\n",
    "LOG_FILE = 'log_prunting.txt'\n",
    "SENSITIVITY = 2 # Sensitivity value that is multiplied to layer's std in order to get threshold value\n",
    "\n",
    "# Control Seed\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Select Device\n",
    "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt DataLoader \n",
    "\n",
    "# Tạo tập dữ liệu MNIST \n",
    "from torchvision import datasets, transforms \n",
    "\n",
    "# Train loader \n",
    "kwargs = {\"num_workers\": 5, \"pin_memory\": True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "\tdatasets.MNIST(\n",
    "\t\t'data', train=True, download=True, \n",
    "\t\ttransform=transforms.Compose([\n",
    "\t\t\ttransforms.ToTensor(), \n",
    "\t\t\ttransforms.Normalize((0.1307, ), (0.3081, ))\n",
    "\t\t])), \n",
    "\tbatch_size=BATCH_SIZE, shuffle=True, **kwargs\n",
    ")\n",
    "# Test loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "\tdatasets.MNIST(\n",
    "\t\t'data', train=False, transform=transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.1307,), (0.3081,))\n",
    "\t])),\n",
    "\tbatch_size=BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (fc1): MaskedLinear(in_features=784, out_features=300, bias=True)\n",
       "  (fc2): MaskedLinear(in_features=300, out_features=100, bias=True)\n",
       "  (fc3): MaskedLinear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet(mask=True).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Định nghĩa AdamOptimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
    "initial_optimizer_state_dict = optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def train(model):\n",
    "\tmodel.train()\n",
    "\tfor epoch in range(EPOCHS):\n",
    "\t\tpbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\t\tfor batch_idx, (data, target) in pbar:\n",
    "\t\t\tdata, target = data.to(device), target.to(device)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput = model(data)\n",
    "\t\t\tloss = F.nll_loss(output, target)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t\n",
    "\t\t\t# Đặt tất cả các gradient tương ứng với các kết nối đã bị cắt tỉa về 0\n",
    "\t\t\t# Hàm này sẽ không chạy trong lần đầu training mà sẽ chạy sau khi mạng \n",
    "\t\t\t# đã được cắt tỉa và cần fine-tuning lại. Giúp optimizer chỉ tối ưu vào \n",
    "\t\t\t# các trọng số chưa được cắt tỉa (quan trọng)\n",
    "\t\t\tfor name, p in model.named_parameters():\n",
    "\t\t\t\tif 'mask' in name:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\ttensor = p.data.cpu().numpy()\n",
    "\t\t\t\tgrad_tensor = p.grad.data.cpu().numpy()\n",
    "\t\t\t\tgrad_tensor = np.where(tensor==0, 0, grad_tensor)\n",
    "\t\t\t\tp.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tif batch_idx % LOG_AFTER == 0:\n",
    "\t\t\t\tdone = batch_idx * len(data)\n",
    "\t\t\t\tpercentage = 100. * batch_idx / len(train_loader)\n",
    "\t\t\t\tpbar.set_description(f'Train Epoch: {epoch} [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)]-----Loss: {loss.item():.6f}')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [58880/60000 ( 98%)]-----Loss: 0.196225: 100%|██████████| 469/469 [00:01<00:00, 342.10it/s]\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]-----Loss: 0.175027: 100%|██████████| 469/469 [00:01<00:00, 365.67it/s]\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]-----Loss: 0.100646: 100%|██████████| 469/469 [00:01<00:00, 368.16it/s]\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]-----Loss: 0.048760: 100%|██████████| 469/469 [00:01<00:00, 384.83it/s]\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]-----Loss: 0.008782: 100%|██████████| 469/469 [00:01<00:00, 399.82it/s]\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]-----Loss: 0.109470: 100%|██████████| 469/469 [00:01<00:00, 330.62it/s]\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]-----Loss: 0.049825: 100%|██████████| 469/469 [00:01<00:00, 323.31it/s]\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]-----Loss: 0.065000: 100%|██████████| 469/469 [00:01<00:00, 317.03it/s]\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]-----Loss: 0.004740: 100%|██████████| 469/469 [00:01<00:00, 315.19it/s]\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]-----Loss: 0.022357: 100%|██████████| 469/469 [00:01<00:00, 323.87it/s]\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]-----Loss: 0.011662: 100%|██████████| 469/469 [00:01<00:00, 324.76it/s]\n",
      "Train Epoch: 11 [58880/60000 ( 98%)]-----Loss: 0.065282: 100%|██████████| 469/469 [00:01<00:00, 318.51it/s]\n",
      "Train Epoch: 12 [58880/60000 ( 98%)]-----Loss: 0.023215: 100%|██████████| 469/469 [00:01<00:00, 317.72it/s]\n",
      "Train Epoch: 13 [58880/60000 ( 98%)]-----Loss: 0.011794: 100%|██████████| 469/469 [00:01<00:00, 323.36it/s]\n",
      "Train Epoch: 14 [58880/60000 ( 98%)]-----Loss: 0.017103: 100%|██████████| 469/469 [00:01<00:00, 321.49it/s]\n",
      "Train Epoch: 15 [58880/60000 ( 98%)]-----Loss: 0.001708: 100%|██████████| 469/469 [00:01<00:00, 330.50it/s]\n",
      "Train Epoch: 16 [58880/60000 ( 98%)]-----Loss: 0.138918: 100%|██████████| 469/469 [00:01<00:00, 325.48it/s]\n",
      "Train Epoch: 17 [58880/60000 ( 98%)]-----Loss: 0.007804: 100%|██████████| 469/469 [00:01<00:00, 323.18it/s]\n",
      "Train Epoch: 18 [58880/60000 ( 98%)]-----Loss: 0.016037: 100%|██████████| 469/469 [00:01<00:00, 324.57it/s]\n",
      "Train Epoch: 19 [58880/60000 ( 98%)]-----Loss: 0.016729: 100%|██████████| 469/469 [00:01<00:00, 321.52it/s]\n",
      "Train Epoch: 20 [58880/60000 ( 98%)]-----Loss: 0.035915: 100%|██████████| 469/469 [00:01<00:00, 322.28it/s]\n",
      "Train Epoch: 21 [58880/60000 ( 98%)]-----Loss: 0.000418: 100%|██████████| 469/469 [00:01<00:00, 329.87it/s]\n",
      "Train Epoch: 22 [58880/60000 ( 98%)]-----Loss: 0.007940: 100%|██████████| 469/469 [00:01<00:00, 325.04it/s]\n",
      "Train Epoch: 23 [58880/60000 ( 98%)]-----Loss: 0.033946: 100%|██████████| 469/469 [00:01<00:00, 324.07it/s]\n",
      "Train Epoch: 24 [58880/60000 ( 98%)]-----Loss: 0.005633: 100%|██████████| 469/469 [00:01<00:00, 328.05it/s]\n",
      "Train Epoch: 25 [58880/60000 ( 98%)]-----Loss: 0.011478: 100%|██████████| 469/469 [00:01<00:00, 323.73it/s]\n",
      "Train Epoch: 26 [58880/60000 ( 98%)]-----Loss: 0.059176: 100%|██████████| 469/469 [00:01<00:00, 315.65it/s]\n",
      "Train Epoch: 27 [58880/60000 ( 98%)]-----Loss: 0.007855: 100%|██████████| 469/469 [00:01<00:00, 315.54it/s]\n",
      "Train Epoch: 28 [58880/60000 ( 98%)]-----Loss: 0.011424: 100%|██████████| 469/469 [00:01<00:00, 317.25it/s]\n",
      "Train Epoch: 29 [58880/60000 ( 98%)]-----Loss: 0.032169: 100%|██████████| 469/469 [00:01<00:00, 323.02it/s]\n",
      "Train Epoch: 30 [58880/60000 ( 98%)]-----Loss: 0.015009: 100%|██████████| 469/469 [00:01<00:00, 318.39it/s]\n",
      "Train Epoch: 31 [58880/60000 ( 98%)]-----Loss: 0.043364: 100%|██████████| 469/469 [00:01<00:00, 321.48it/s]\n",
      "Train Epoch: 32 [58880/60000 ( 98%)]-----Loss: 0.003740: 100%|██████████| 469/469 [00:01<00:00, 326.09it/s]\n",
      "Train Epoch: 33 [58880/60000 ( 98%)]-----Loss: 0.014778: 100%|██████████| 469/469 [00:01<00:00, 322.86it/s]\n",
      "Train Epoch: 34 [58880/60000 ( 98%)]-----Loss: 0.009547: 100%|██████████| 469/469 [00:01<00:00, 317.49it/s]\n",
      "Train Epoch: 35 [58880/60000 ( 98%)]-----Loss: 0.003581: 100%|██████████| 469/469 [00:01<00:00, 315.39it/s]\n",
      "Train Epoch: 36 [58880/60000 ( 98%)]-----Loss: 0.011422: 100%|██████████| 469/469 [00:01<00:00, 315.47it/s]\n",
      "Train Epoch: 37 [58880/60000 ( 98%)]-----Loss: 0.021245: 100%|██████████| 469/469 [00:01<00:00, 318.89it/s]\n",
      "Train Epoch: 38 [58880/60000 ( 98%)]-----Loss: 0.003392: 100%|██████████| 469/469 [00:01<00:00, 321.53it/s]\n",
      "Train Epoch: 39 [58880/60000 ( 98%)]-----Loss: 0.007008: 100%|██████████| 469/469 [00:01<00:00, 388.39it/s]\n",
      "Train Epoch: 40 [58880/60000 ( 98%)]-----Loss: 0.004392: 100%|██████████| 469/469 [00:01<00:00, 388.95it/s]\n",
      "Train Epoch: 41 [58880/60000 ( 98%)]-----Loss: 0.005433: 100%|██████████| 469/469 [00:01<00:00, 394.62it/s]\n",
      "Train Epoch: 42 [58880/60000 ( 98%)]-----Loss: 0.004069: 100%|██████████| 469/469 [00:01<00:00, 335.81it/s]\n",
      "Train Epoch: 43 [58880/60000 ( 98%)]-----Loss: 0.014500: 100%|██████████| 469/469 [00:01<00:00, 320.14it/s]\n",
      "Train Epoch: 44 [58880/60000 ( 98%)]-----Loss: 0.007688: 100%|██████████| 469/469 [00:01<00:00, 330.40it/s]\n",
      "Train Epoch: 45 [58880/60000 ( 98%)]-----Loss: 0.017593: 100%|██████████| 469/469 [00:01<00:00, 328.50it/s]\n",
      "Train Epoch: 46 [58880/60000 ( 98%)]-----Loss: 0.004583: 100%|██████████| 469/469 [00:01<00:00, 320.34it/s]\n",
      "Train Epoch: 47 [58880/60000 ( 98%)]-----Loss: 0.004411: 100%|██████████| 469/469 [00:01<00:00, 323.50it/s]\n",
      "Train Epoch: 48 [58880/60000 ( 98%)]-----Loss: 0.008536: 100%|██████████| 469/469 [00:01<00:00, 328.55it/s]\n",
      "Train Epoch: 49 [58880/60000 ( 98%)]-----Loss: 0.001740: 100%|██████████| 469/469 [00:01<00:00, 325.22it/s]\n",
      "Train Epoch: 50 [58880/60000 ( 98%)]-----Loss: 0.022133: 100%|██████████| 469/469 [00:01<00:00, 324.95it/s]\n",
      "Train Epoch: 51 [58880/60000 ( 98%)]-----Loss: 0.001809: 100%|██████████| 469/469 [00:01<00:00, 325.44it/s]\n",
      "Train Epoch: 52 [58880/60000 ( 98%)]-----Loss: 0.010582: 100%|██████████| 469/469 [00:01<00:00, 324.20it/s]\n",
      "Train Epoch: 53 [58880/60000 ( 98%)]-----Loss: 0.011001: 100%|██████████| 469/469 [00:01<00:00, 321.42it/s]\n",
      "Train Epoch: 54 [58880/60000 ( 98%)]-----Loss: 0.041131: 100%|██████████| 469/469 [00:01<00:00, 326.39it/s]\n",
      "Train Epoch: 55 [58880/60000 ( 98%)]-----Loss: 0.006837: 100%|██████████| 469/469 [00:01<00:00, 324.87it/s]\n",
      "Train Epoch: 56 [58880/60000 ( 98%)]-----Loss: 0.001430: 100%|██████████| 469/469 [00:01<00:00, 327.30it/s]\n",
      "Train Epoch: 57 [58880/60000 ( 98%)]-----Loss: 0.007215: 100%|██████████| 469/469 [00:01<00:00, 326.38it/s]\n",
      "Train Epoch: 58 [58880/60000 ( 98%)]-----Loss: 0.008938: 100%|██████████| 469/469 [00:01<00:00, 326.08it/s]\n",
      "Train Epoch: 59 [58880/60000 ( 98%)]-----Loss: 0.001533: 100%|██████████| 469/469 [00:01<00:00, 322.88it/s]\n",
      "Train Epoch: 60 [58880/60000 ( 98%)]-----Loss: 0.000433: 100%|██████████| 469/469 [00:01<00:00, 319.43it/s]\n",
      "Train Epoch: 61 [58880/60000 ( 98%)]-----Loss: 0.001797: 100%|██████████| 469/469 [00:01<00:00, 326.54it/s]\n",
      "Train Epoch: 62 [58880/60000 ( 98%)]-----Loss: 0.008949: 100%|██████████| 469/469 [00:01<00:00, 321.75it/s]\n",
      "Train Epoch: 63 [58880/60000 ( 98%)]-----Loss: 0.003771: 100%|██████████| 469/469 [00:01<00:00, 318.22it/s]\n",
      "Train Epoch: 64 [58880/60000 ( 98%)]-----Loss: 0.009114: 100%|██████████| 469/469 [00:01<00:00, 331.49it/s]\n",
      "Train Epoch: 65 [58880/60000 ( 98%)]-----Loss: 0.002618: 100%|██████████| 469/469 [00:01<00:00, 325.50it/s]\n",
      "Train Epoch: 66 [58880/60000 ( 98%)]-----Loss: 0.002650: 100%|██████████| 469/469 [00:01<00:00, 326.11it/s]\n",
      "Train Epoch: 67 [58880/60000 ( 98%)]-----Loss: 0.009458: 100%|██████████| 469/469 [00:01<00:00, 327.95it/s]\n",
      "Train Epoch: 68 [58880/60000 ( 98%)]-----Loss: 0.000650: 100%|██████████| 469/469 [00:01<00:00, 324.69it/s]\n",
      "Train Epoch: 69 [58880/60000 ( 98%)]-----Loss: 0.002804: 100%|██████████| 469/469 [00:01<00:00, 332.48it/s]\n",
      "Train Epoch: 70 [58880/60000 ( 98%)]-----Loss: 0.016158: 100%|██████████| 469/469 [00:01<00:00, 332.76it/s]\n",
      "Train Epoch: 71 [58880/60000 ( 98%)]-----Loss: 0.005554: 100%|██████████| 469/469 [00:01<00:00, 316.92it/s]\n",
      "Train Epoch: 72 [58880/60000 ( 98%)]-----Loss: 0.009821: 100%|██████████| 469/469 [00:01<00:00, 329.49it/s]\n",
      "Train Epoch: 73 [58880/60000 ( 98%)]-----Loss: 0.016388: 100%|██████████| 469/469 [00:01<00:00, 317.01it/s]\n",
      "Train Epoch: 74 [58880/60000 ( 98%)]-----Loss: 0.000355: 100%|██████████| 469/469 [00:01<00:00, 321.19it/s]\n",
      "Train Epoch: 75 [58880/60000 ( 98%)]-----Loss: 0.001049: 100%|██████████| 469/469 [00:01<00:00, 316.51it/s]\n",
      "Train Epoch: 76 [58880/60000 ( 98%)]-----Loss: 0.006716: 100%|██████████| 469/469 [00:01<00:00, 318.44it/s]\n",
      "Train Epoch: 77 [58880/60000 ( 98%)]-----Loss: 0.001710: 100%|██████████| 469/469 [00:01<00:00, 314.36it/s]\n",
      "Train Epoch: 78 [58880/60000 ( 98%)]-----Loss: 0.008147: 100%|██████████| 469/469 [00:01<00:00, 322.92it/s]\n",
      "Train Epoch: 79 [58880/60000 ( 98%)]-----Loss: 0.051687: 100%|██████████| 469/469 [00:01<00:00, 322.57it/s]\n",
      "Train Epoch: 80 [58880/60000 ( 98%)]-----Loss: 0.005867: 100%|██████████| 469/469 [00:01<00:00, 323.63it/s]\n",
      "Train Epoch: 81 [58880/60000 ( 98%)]-----Loss: 0.002381: 100%|██████████| 469/469 [00:01<00:00, 327.36it/s]\n",
      "Train Epoch: 82 [58880/60000 ( 98%)]-----Loss: 0.076694: 100%|██████████| 469/469 [00:01<00:00, 326.06it/s]\n",
      "Train Epoch: 83 [58880/60000 ( 98%)]-----Loss: 0.000825: 100%|██████████| 469/469 [00:01<00:00, 315.51it/s]\n",
      "Train Epoch: 84 [58880/60000 ( 98%)]-----Loss: 0.001707: 100%|██████████| 469/469 [00:01<00:00, 317.96it/s]\n",
      "Train Epoch: 85 [58880/60000 ( 98%)]-----Loss: 0.003088: 100%|██████████| 469/469 [00:01<00:00, 325.12it/s]\n",
      "Train Epoch: 86 [58880/60000 ( 98%)]-----Loss: 0.001921: 100%|██████████| 469/469 [00:01<00:00, 317.16it/s]\n",
      "Train Epoch: 87 [58880/60000 ( 98%)]-----Loss: 0.000949: 100%|██████████| 469/469 [00:01<00:00, 336.94it/s]\n",
      "Train Epoch: 88 [58880/60000 ( 98%)]-----Loss: 0.042485: 100%|██████████| 469/469 [00:01<00:00, 385.64it/s]\n",
      "Train Epoch: 89 [58880/60000 ( 98%)]-----Loss: 0.039298: 100%|██████████| 469/469 [00:01<00:00, 388.16it/s]\n",
      "Train Epoch: 90 [58880/60000 ( 98%)]-----Loss: 0.001109: 100%|██████████| 469/469 [00:01<00:00, 341.97it/s]\n",
      "Train Epoch: 91 [58880/60000 ( 98%)]-----Loss: 0.002645: 100%|██████████| 469/469 [00:01<00:00, 325.34it/s]\n",
      "Train Epoch: 92 [58880/60000 ( 98%)]-----Loss: 0.000792: 100%|██████████| 469/469 [00:01<00:00, 327.72it/s]\n",
      "Train Epoch: 93 [58880/60000 ( 98%)]-----Loss: 0.009254: 100%|██████████| 469/469 [00:01<00:00, 331.35it/s]\n",
      "Train Epoch: 94 [58880/60000 ( 98%)]-----Loss: 0.026313: 100%|██████████| 469/469 [00:01<00:00, 318.12it/s]\n",
      "Train Epoch: 95 [58880/60000 ( 98%)]-----Loss: 0.000535: 100%|██████████| 469/469 [00:01<00:00, 334.20it/s]\n",
      "Train Epoch: 96 [58880/60000 ( 98%)]-----Loss: 0.003933: 100%|██████████| 469/469 [00:01<00:00, 331.06it/s]\n",
      "Train Epoch: 97 [58880/60000 ( 98%)]-----Loss: 0.003147: 100%|██████████| 469/469 [00:01<00:00, 312.04it/s]\n",
      "Train Epoch: 98 [58880/60000 ( 98%)]-----Loss: 0.015965: 100%|██████████| 469/469 [00:01<00:00, 319.19it/s]\n",
      "Train Epoch: 99 [58880/60000 ( 98%)]-----Loss: 0.011012: 100%|██████████| 469/469 [00:01<00:00, 314.70it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "def test(model):\n",
    "\tstart_time = time()\n",
    "\tmodel.eval()\n",
    "\ttest_loss = 0\n",
    "\tcorrect = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data, target in test_loader:\n",
    "\t\t\tdata, target = data.to(device), target.to(device)\n",
    "\t\t\toutput = model(data)\n",
    "\t\t\ttest_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "\t\t\tpred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\t\t\tcorrect += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "\t\ttest_loss /= len(test_loader.dataset)\n",
    "\t\taccuracy = 100. * correct / len(test_loader.dataset)\n",
    "\t\tprint(f'>>> Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%). Total time = {time() - start_time}')\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Test set: Average loss: 0.0809, Accuracy: 9814/10000 (98.14%). Total time = 0.16672825813293457\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu lại giá trị log vào file để theo dõi\n",
    "def save_log(filename, content):\n",
    "\twith open(filename, 'a') as f:\n",
    "\t\tcontent += \"\\n\"\n",
    "\t\tf.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log(LOG_FILE, f\"initial_accuracy {accuracy}\")\n",
    "torch.save(model, f\"save_models/initial_model.ptmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính số lượng non-zeros parameters\n",
    "def print_nonzeros(model):\n",
    "\t\"\"\"Hiển thị số lượng trọng số non-zeros của model (mô hình)\"\"\"\n",
    "\tnonzero = total = 0\n",
    "\tfor name, p in model.named_parameters():\n",
    "\t\tif 'mask' in name:\n",
    "\t\t\tcontinue\n",
    "\t\ttensor = p.data.cpu().numpy()\n",
    "\t\tnz_count = np.count_nonzero(tensor)\n",
    "\t\ttotal_params = np.prod(tensor.shape)\n",
    "\t\tnonzero += nz_count\n",
    "\t\ttotal += total_params\n",
    "\t\tprint(f'{name:20} | nonzeros = {nz_count:7} / {total_params:7} ({100 * nz_count / total_params:6.2f}%) | total_pruned = {total_params - nz_count :7} | shape = {tensor.shape}')\n",
    "\tprint(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =  235200 /  235200 (100.00%) | total_pruned =       0 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =   30000 /   30000 (100.00%) | total_pruned =       0 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =    1000 /    1000 (100.00%) | total_pruned =       0 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 266610, pruned : 0, total: 266610, Compression rate :       1.00x  (  0.00% pruned)\n"
     ]
    }
   ],
   "source": [
    "print_nonzeros(model)\n",
    "# Có thể thấy rằng khi chưa được cắt tỉa thì mạng này\n",
    "# có toàn bộ các weight là khác 0. Tức là hiện tại chưa \n",
    "# có weight nào được cắt tỉa (0.0% pruned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cắt tỉa với ngưỡng: 0.07601112872362137 cho layer fc1\n",
      ">>> Cắt tỉa với ngưỡng: 0.11727409809827805 cho layer fc2\n",
      ">>> Cắt tỉa với ngưỡng: 0.37368443608283997 cho layer fc3\n"
     ]
    }
   ],
   "source": [
    "# Tiến hành cắt tỉa\n",
    "model.prune_by_std(SENSITIVITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Test set: Average loss: 0.9760, Accuracy: 6674/10000 (66.74%). Total time = 0.15550613403320312\n"
     ]
    }
   ],
   "source": [
    "# Chạy test lại độ chính xác sau khi cắt tỉa\n",
    "accuracy = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight           | nonzeros =   13450 /  235200 (  5.72%) | total_pruned =  221750 | shape = (300, 784)\n",
      "fc1.bias             | nonzeros =     300 /     300 (100.00%) | total_pruned =       0 | shape = (300,)\n",
      "fc2.weight           | nonzeros =    2103 /   30000 (  7.01%) | total_pruned =   27897 | shape = (100, 300)\n",
      "fc2.bias             | nonzeros =     100 /     100 (100.00%) | total_pruned =       0 | shape = (100,)\n",
      "fc3.weight           | nonzeros =      67 /    1000 (  6.70%) | total_pruned =     933 | shape = (10, 100)\n",
      "fc3.bias             | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 16030, pruned : 250580, total: 266610, Compression rate :      16.63x  ( 93.99% pruned)\n"
     ]
    }
   ],
   "source": [
    "# Lưu kết quả vào log file và kiểm tra lại số lượng tham số của mạng\n",
    "save_log(LOG_FILE, f\"accuracy_after_pruning {accuracy}\")\n",
    "print_nonzeros(model)\n",
    "# Nhận xét: Mô hình giảm đi độ chính xác khá nhiều, từ 98.12% xuống còn 60.01%\n",
    "# Trong khi số lượng tham số bị cắt tỉa là 94.11% (pruned) tương ứng tỷ lệ nén \n",
    "# khoảng 16.98x lần. Tiếp theo ta cần training lại PrunedNetwork (mạng sau khi \n",
    "# cắt tỉa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [58880/60000 ( 98%)]-----Loss: 0.019772: 100%|██████████| 469/469 [00:01<00:00, 315.47it/s]\n",
      "Train Epoch: 1 [58880/60000 ( 98%)]-----Loss: 0.073803: 100%|██████████| 469/469 [00:01<00:00, 314.03it/s]\n",
      "Train Epoch: 2 [58880/60000 ( 98%)]-----Loss: 0.007872: 100%|██████████| 469/469 [00:01<00:00, 317.05it/s]\n",
      "Train Epoch: 3 [58880/60000 ( 98%)]-----Loss: 0.022281: 100%|██████████| 469/469 [00:01<00:00, 312.70it/s]\n",
      "Train Epoch: 4 [58880/60000 ( 98%)]-----Loss: 0.018000: 100%|██████████| 469/469 [00:01<00:00, 316.28it/s]\n",
      "Train Epoch: 5 [58880/60000 ( 98%)]-----Loss: 0.012123: 100%|██████████| 469/469 [00:01<00:00, 314.01it/s]\n",
      "Train Epoch: 6 [58880/60000 ( 98%)]-----Loss: 0.014565: 100%|██████████| 469/469 [00:01<00:00, 309.40it/s]\n",
      "Train Epoch: 7 [58880/60000 ( 98%)]-----Loss: 0.022879: 100%|██████████| 469/469 [00:01<00:00, 311.53it/s]\n",
      "Train Epoch: 8 [58880/60000 ( 98%)]-----Loss: 0.003166: 100%|██████████| 469/469 [00:01<00:00, 311.71it/s]\n",
      "Train Epoch: 9 [58880/60000 ( 98%)]-----Loss: 0.004190: 100%|██████████| 469/469 [00:01<00:00, 304.84it/s]\n",
      "Train Epoch: 10 [58880/60000 ( 98%)]-----Loss: 0.002556: 100%|██████████| 469/469 [00:01<00:00, 363.34it/s]\n",
      "Train Epoch: 11 [58880/60000 ( 98%)]-----Loss: 0.005618: 100%|██████████| 469/469 [00:01<00:00, 385.28it/s]\n",
      "Train Epoch: 12 [58880/60000 ( 98%)]-----Loss: 0.003802: 100%|██████████| 469/469 [00:01<00:00, 363.57it/s]\n",
      "Train Epoch: 13 [58880/60000 ( 98%)]-----Loss: 0.003466: 100%|██████████| 469/469 [00:01<00:00, 315.87it/s]\n",
      "Train Epoch: 14 [58880/60000 ( 98%)]-----Loss: 0.002293: 100%|██████████| 469/469 [00:01<00:00, 323.61it/s]\n",
      "Train Epoch: 15 [58880/60000 ( 98%)]-----Loss: 0.013451: 100%|██████████| 469/469 [00:01<00:00, 317.42it/s]\n",
      "Train Epoch: 16 [58880/60000 ( 98%)]-----Loss: 0.005464: 100%|██████████| 469/469 [00:01<00:00, 315.93it/s]\n",
      "Train Epoch: 17 [58880/60000 ( 98%)]-----Loss: 0.015341: 100%|██████████| 469/469 [00:01<00:00, 316.42it/s]\n",
      "Train Epoch: 18 [58880/60000 ( 98%)]-----Loss: 0.009857: 100%|██████████| 469/469 [00:01<00:00, 312.75it/s]\n",
      "Train Epoch: 19 [58880/60000 ( 98%)]-----Loss: 0.004149: 100%|██████████| 469/469 [00:01<00:00, 319.68it/s]\n",
      "Train Epoch: 20 [58880/60000 ( 98%)]-----Loss: 0.002982: 100%|██████████| 469/469 [00:01<00:00, 315.21it/s]\n",
      "Train Epoch: 21 [58880/60000 ( 98%)]-----Loss: 0.002078: 100%|██████████| 469/469 [00:01<00:00, 302.41it/s]\n",
      "Train Epoch: 22 [58880/60000 ( 98%)]-----Loss: 0.011289: 100%|██████████| 469/469 [00:01<00:00, 313.99it/s]\n",
      "Train Epoch: 23 [58880/60000 ( 98%)]-----Loss: 0.002180: 100%|██████████| 469/469 [00:01<00:00, 317.53it/s]\n",
      "Train Epoch: 24 [58880/60000 ( 98%)]-----Loss: 0.011061: 100%|██████████| 469/469 [00:01<00:00, 313.39it/s]\n",
      "Train Epoch: 25 [58880/60000 ( 98%)]-----Loss: 0.003061: 100%|██████████| 469/469 [00:01<00:00, 317.91it/s]\n",
      "Train Epoch: 26 [58880/60000 ( 98%)]-----Loss: 0.006119: 100%|██████████| 469/469 [00:01<00:00, 316.42it/s]\n",
      "Train Epoch: 27 [58880/60000 ( 98%)]-----Loss: 0.004568: 100%|██████████| 469/469 [00:01<00:00, 317.21it/s]\n",
      "Train Epoch: 28 [58880/60000 ( 98%)]-----Loss: 0.010943: 100%|██████████| 469/469 [00:01<00:00, 322.14it/s]\n",
      "Train Epoch: 29 [58880/60000 ( 98%)]-----Loss: 0.001975: 100%|██████████| 469/469 [00:01<00:00, 320.70it/s]\n",
      "Train Epoch: 30 [58880/60000 ( 98%)]-----Loss: 0.019233: 100%|██████████| 469/469 [00:01<00:00, 311.43it/s]\n",
      "Train Epoch: 31 [58880/60000 ( 98%)]-----Loss: 0.007064: 100%|██████████| 469/469 [00:01<00:00, 317.12it/s]\n",
      "Train Epoch: 32 [58880/60000 ( 98%)]-----Loss: 0.010398: 100%|██████████| 469/469 [00:01<00:00, 312.05it/s]\n",
      "Train Epoch: 33 [58880/60000 ( 98%)]-----Loss: 0.009857: 100%|██████████| 469/469 [00:01<00:00, 308.87it/s]\n",
      "Train Epoch: 34 [58880/60000 ( 98%)]-----Loss: 0.006533: 100%|██████████| 469/469 [00:01<00:00, 317.55it/s]\n",
      "Train Epoch: 35 [58880/60000 ( 98%)]-----Loss: 0.003273: 100%|██████████| 469/469 [00:01<00:00, 307.47it/s]\n",
      "Train Epoch: 36 [58880/60000 ( 98%)]-----Loss: 0.016451: 100%|██████████| 469/469 [00:01<00:00, 311.27it/s]\n",
      "Train Epoch: 37 [58880/60000 ( 98%)]-----Loss: 0.007730: 100%|██████████| 469/469 [00:01<00:00, 312.86it/s]\n",
      "Train Epoch: 38 [58880/60000 ( 98%)]-----Loss: 0.002692: 100%|██████████| 469/469 [00:01<00:00, 310.49it/s]\n",
      "Train Epoch: 39 [58880/60000 ( 98%)]-----Loss: 0.007915: 100%|██████████| 469/469 [00:01<00:00, 310.82it/s]\n",
      "Train Epoch: 40 [58880/60000 ( 98%)]-----Loss: 0.016697: 100%|██████████| 469/469 [00:01<00:00, 318.75it/s]\n",
      "Train Epoch: 41 [58880/60000 ( 98%)]-----Loss: 0.007560: 100%|██████████| 469/469 [00:01<00:00, 318.64it/s]\n",
      "Train Epoch: 42 [58880/60000 ( 98%)]-----Loss: 0.004927: 100%|██████████| 469/469 [00:01<00:00, 318.41it/s]\n",
      "Train Epoch: 43 [58880/60000 ( 98%)]-----Loss: 0.002740: 100%|██████████| 469/469 [00:01<00:00, 320.51it/s]\n",
      "Train Epoch: 44 [58880/60000 ( 98%)]-----Loss: 0.006916: 100%|██████████| 469/469 [00:01<00:00, 316.21it/s]\n",
      "Train Epoch: 45 [58880/60000 ( 98%)]-----Loss: 0.001761: 100%|██████████| 469/469 [00:01<00:00, 316.19it/s]\n",
      "Train Epoch: 46 [58880/60000 ( 98%)]-----Loss: 0.003100: 100%|██████████| 469/469 [00:01<00:00, 310.55it/s]\n",
      "Train Epoch: 47 [58880/60000 ( 98%)]-----Loss: 0.001613: 100%|██████████| 469/469 [00:01<00:00, 316.65it/s]\n",
      "Train Epoch: 48 [58880/60000 ( 98%)]-----Loss: 0.003848: 100%|██████████| 469/469 [00:01<00:00, 313.37it/s]\n",
      "Train Epoch: 49 [58880/60000 ( 98%)]-----Loss: 0.008540: 100%|██████████| 469/469 [00:01<00:00, 303.12it/s]\n",
      "Train Epoch: 50 [58880/60000 ( 98%)]-----Loss: 0.002999: 100%|██████████| 469/469 [00:01<00:00, 297.38it/s]\n",
      "Train Epoch: 51 [58880/60000 ( 98%)]-----Loss: 0.001172: 100%|██████████| 469/469 [00:01<00:00, 315.87it/s]\n",
      "Train Epoch: 52 [58880/60000 ( 98%)]-----Loss: 0.010649: 100%|██████████| 469/469 [00:01<00:00, 312.28it/s]\n",
      "Train Epoch: 53 [58880/60000 ( 98%)]-----Loss: 0.005126: 100%|██████████| 469/469 [00:01<00:00, 350.20it/s]\n",
      "Train Epoch: 54 [58880/60000 ( 98%)]-----Loss: 0.001752: 100%|██████████| 469/469 [00:01<00:00, 371.37it/s]\n",
      "Train Epoch: 55 [58880/60000 ( 98%)]-----Loss: 0.000666: 100%|██████████| 469/469 [00:01<00:00, 364.30it/s]\n",
      "Train Epoch: 56 [58880/60000 ( 98%)]-----Loss: 0.000897: 100%|██████████| 469/469 [00:01<00:00, 310.94it/s]\n",
      "Train Epoch: 57 [58880/60000 ( 98%)]-----Loss: 0.027593: 100%|██████████| 469/469 [00:01<00:00, 311.68it/s]\n",
      "Train Epoch: 58 [58880/60000 ( 98%)]-----Loss: 0.003442: 100%|██████████| 469/469 [00:01<00:00, 314.91it/s]\n",
      "Train Epoch: 59 [58880/60000 ( 98%)]-----Loss: 0.001798: 100%|██████████| 469/469 [00:01<00:00, 321.89it/s]\n",
      "Train Epoch: 60 [58880/60000 ( 98%)]-----Loss: 0.001740: 100%|██████████| 469/469 [00:01<00:00, 313.28it/s]\n",
      "Train Epoch: 61 [58880/60000 ( 98%)]-----Loss: 0.003519: 100%|██████████| 469/469 [00:01<00:00, 383.52it/s]\n",
      "Train Epoch: 62 [58880/60000 ( 98%)]-----Loss: 0.009054: 100%|██████████| 469/469 [00:01<00:00, 380.92it/s]\n",
      "Train Epoch: 63 [58880/60000 ( 98%)]-----Loss: 0.001446: 100%|██████████| 469/469 [00:01<00:00, 359.10it/s]\n",
      "Train Epoch: 64 [58880/60000 ( 98%)]-----Loss: 0.000801: 100%|██████████| 469/469 [00:01<00:00, 317.86it/s]\n",
      "Train Epoch: 65 [58880/60000 ( 98%)]-----Loss: 0.001447: 100%|██████████| 469/469 [00:01<00:00, 319.84it/s]\n",
      "Train Epoch: 66 [58880/60000 ( 98%)]-----Loss: 0.004723: 100%|██████████| 469/469 [00:01<00:00, 314.76it/s]\n",
      "Train Epoch: 67 [58880/60000 ( 98%)]-----Loss: 0.011923: 100%|██████████| 469/469 [00:01<00:00, 315.09it/s]\n",
      "Train Epoch: 68 [58880/60000 ( 98%)]-----Loss: 0.001198: 100%|██████████| 469/469 [00:01<00:00, 316.99it/s]\n",
      "Train Epoch: 69 [58880/60000 ( 98%)]-----Loss: 0.005860: 100%|██████████| 469/469 [00:01<00:00, 315.70it/s]\n",
      "Train Epoch: 70 [58880/60000 ( 98%)]-----Loss: 0.006536: 100%|██████████| 469/469 [00:01<00:00, 305.93it/s]\n",
      "Train Epoch: 71 [58880/60000 ( 98%)]-----Loss: 0.001685: 100%|██████████| 469/469 [00:01<00:00, 307.22it/s]\n",
      "Train Epoch: 72 [58880/60000 ( 98%)]-----Loss: 0.005498: 100%|██████████| 469/469 [00:01<00:00, 312.53it/s]\n",
      "Train Epoch: 73 [58880/60000 ( 98%)]-----Loss: 0.014529: 100%|██████████| 469/469 [00:01<00:00, 309.22it/s]\n",
      "Train Epoch: 74 [58880/60000 ( 98%)]-----Loss: 0.001980: 100%|██████████| 469/469 [00:01<00:00, 304.50it/s]\n",
      "Train Epoch: 75 [58880/60000 ( 98%)]-----Loss: 0.000590: 100%|██████████| 469/469 [00:01<00:00, 299.48it/s]\n",
      "Train Epoch: 76 [58880/60000 ( 98%)]-----Loss: 0.001867: 100%|██████████| 469/469 [00:01<00:00, 303.43it/s]\n",
      "Train Epoch: 77 [58880/60000 ( 98%)]-----Loss: 0.005268: 100%|██████████| 469/469 [00:01<00:00, 303.12it/s]\n",
      "Train Epoch: 78 [58880/60000 ( 98%)]-----Loss: 0.001490: 100%|██████████| 469/469 [00:01<00:00, 306.62it/s]\n",
      "Train Epoch: 79 [58880/60000 ( 98%)]-----Loss: 0.000319: 100%|██████████| 469/469 [00:01<00:00, 311.96it/s]\n",
      "Train Epoch: 80 [58880/60000 ( 98%)]-----Loss: 0.002336: 100%|██████████| 469/469 [00:01<00:00, 312.58it/s]\n",
      "Train Epoch: 81 [58880/60000 ( 98%)]-----Loss: 0.003039: 100%|██████████| 469/469 [00:01<00:00, 317.25it/s]\n",
      "Train Epoch: 82 [58880/60000 ( 98%)]-----Loss: 0.004210: 100%|██████████| 469/469 [00:01<00:00, 313.58it/s]\n",
      "Train Epoch: 83 [58880/60000 ( 98%)]-----Loss: 0.008742: 100%|██████████| 469/469 [00:01<00:00, 315.54it/s]\n",
      "Train Epoch: 84 [58880/60000 ( 98%)]-----Loss: 0.002763: 100%|██████████| 469/469 [00:01<00:00, 317.51it/s]\n",
      "Train Epoch: 85 [58880/60000 ( 98%)]-----Loss: 0.001826: 100%|██████████| 469/469 [00:01<00:00, 312.63it/s]\n",
      "Train Epoch: 86 [58880/60000 ( 98%)]-----Loss: 0.002358: 100%|██████████| 469/469 [00:01<00:00, 313.00it/s]\n",
      "Train Epoch: 87 [58880/60000 ( 98%)]-----Loss: 0.002953: 100%|██████████| 469/469 [00:01<00:00, 311.80it/s]\n",
      "Train Epoch: 88 [58880/60000 ( 98%)]-----Loss: 0.004129: 100%|██████████| 469/469 [00:01<00:00, 317.22it/s]\n",
      "Train Epoch: 89 [58880/60000 ( 98%)]-----Loss: 0.002510: 100%|██████████| 469/469 [00:01<00:00, 316.23it/s]\n",
      "Train Epoch: 90 [58880/60000 ( 98%)]-----Loss: 0.002495: 100%|██████████| 469/469 [00:01<00:00, 316.55it/s]\n",
      "Train Epoch: 91 [58880/60000 ( 98%)]-----Loss: 0.003345: 100%|██████████| 469/469 [00:01<00:00, 316.11it/s]\n",
      "Train Epoch: 92 [58880/60000 ( 98%)]-----Loss: 0.017737: 100%|██████████| 469/469 [00:01<00:00, 309.67it/s]\n",
      "Train Epoch: 93 [58880/60000 ( 98%)]-----Loss: 0.002750: 100%|██████████| 469/469 [00:01<00:00, 310.00it/s]\n",
      "Train Epoch: 94 [58880/60000 ( 98%)]-----Loss: 0.018174: 100%|██████████| 469/469 [00:01<00:00, 307.26it/s]\n",
      "Train Epoch: 95 [58880/60000 ( 98%)]-----Loss: 0.000874: 100%|██████████| 469/469 [00:01<00:00, 313.10it/s]\n",
      "Train Epoch: 96 [58880/60000 ( 98%)]-----Loss: 0.010900: 100%|██████████| 469/469 [00:01<00:00, 309.14it/s]\n",
      "Train Epoch: 97 [58880/60000 ( 98%)]-----Loss: 0.045150: 100%|██████████| 469/469 [00:01<00:00, 309.98it/s]\n",
      "Train Epoch: 98 [58880/60000 ( 98%)]-----Loss: 0.001311: 100%|██████████| 469/469 [00:01<00:00, 318.41it/s]\n",
      "Train Epoch: 99 [58880/60000 ( 98%)]-----Loss: 0.001593: 100%|██████████| 469/469 [00:01<00:00, 316.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Retraining PrunedNetwork (mạng sau khi cắt tỉa)\n",
    "optimizer.load_state_dict(initial_optimizer_state_dict) # Reset the optimizer\n",
    "\n",
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Test set: Average loss: 0.0674, Accuracy: 9808/10000 (98.08%). Total time = 0.15483522415161133\n"
     ]
    }
   ],
   "source": [
    "# Chạy test lại độ chính xác của PrunedNetwork (mạng sau khi cắt tỉa)\n",
    "accuracy = test(model)\n",
    "# Nhận xét: Độ chính xác sau khi retraining PrunedNetwork (mạng sau khi \n",
    "# cắt tỉa, với tỷ lệ nén 16.63x) đã tăng từ 66.74% lên 98.08%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log(LOG_FILE, f\"accuracy_after_retraining {accuracy}\")\n",
    "torch.save(model, f\"save_models/model_after_retraining.ptmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (fc1): MaskedLinear(in_features=784, out_features=300, bias=True)\n",
       "  (fc2): MaskedLinear(in_features=300, out_features=100, bias=True)\n",
       "  (fc3): MaskedLinear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tiếp theo để tiến hành tăng tỉ số nén chúng ta sẽ đến với phần lượng tử hóa và share weight \n",
    "\n",
    "# Lượng tử hoá và share weight\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "\n",
    "def apply_weight_sharing(model, bits=5):\n",
    "\tfor module in model.children():\n",
    "\t\tdevive = module.weight.device\n",
    "\t\tweight = module.weight.data.cpu().numpy()\n",
    "\t\tshape = weight.shape\n",
    "\t\t# Note:\n",
    "\t\t# \tLưu trữ dưới dạng compressed sparse row (CSR) hoặc compressed sparse column \n",
    "\t\t# \t(CSC) format là hai format để lưu trữ ma trận thưa nhằm tính toán được dễ \n",
    "\t\t# \tdàng do tiết kiệm về bộ nhớ. Vì weight là là một ma trận rất thưa với 94% \n",
    "\t\t# \tcác trọng số là khác 0, nên cần phải có một cấu trúc phù hợp để lưu trữ và \n",
    "\t\t# \ttính toán.\n",
    "\t\tmat = csr_matrix(weight) if shape[0] < shape[1] else csc_matrix(weight)\n",
    "\t\t# Sử dụng Kmean để phân cụm, được thực hiện như sau\n",
    "\t\tmin_ = min(mat.data)\n",
    "\t\tmax_ = max(mat.data)\n",
    "\t\tspace = np.linspace(min_, max_, num=2**bits)\n",
    "\t\tkmeans = KMeans(\n",
    "\t\t\tn_clusters=len(space), \n",
    "\t\t\tinit=space.reshape(-1,1), n_init=1, \n",
    "\t\t\t# precompute_distances=True, # Từ 0.24.0 trở đi, loại bỏ do đã được tối ưu hoá trực tiếp trong hàm KMeans\n",
    "\t\t\t# algorithm=\"full\" # Từ 0.24.0 trở đi, tham số algorithm chỉ chấp nhận các giá trị 'elkan' và 'lloyd'\n",
    "\t\t\talgorithm='lloyd'\n",
    "\t\t)\n",
    "\t\tkmeans.fit(mat.data.reshape(-1,1))\n",
    "\t\t# Ở đây số lượng bits được sử dụng để lưu trữ các giá trị weight là 5. Nên ta \n",
    "\t\t# có tối đa là 2^5=32 cụm của K-means. Sau khi thực hiện phân cụm xong thì ta \n",
    "\t\t# tiến hành share lại centroid vào các vị trí weight bằng hàm.\n",
    "\t\tnew_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(-1)\n",
    "\t\tmat.data = new_weight\n",
    "\t\tmodule.weight.data = torch.from_numpy(mat.toarray()).to(devive)\n",
    "\treturn model\n",
    "\n",
    "apply_weight_sharing(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Test set: Average loss: 0.0688, Accuracy: 9802/10000 (98.02%). Total time = 0.16124367713928223\n"
     ]
    }
   ],
   "source": [
    "# Sau khi tiến hành share weight, ta cần tính toán lại accuracy\n",
    "accuracy = test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foxer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
