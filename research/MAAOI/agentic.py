import re
import json
import fire 
import logging
import streamlit as st 



from pydantic import BaseModel, TypeAdapter
from typing_extensions import List, Dict, Type



from langchain_community.chat_message_histories import PostgresChatMessageHistory
from langchain_core.language_models import LanguageModelInput
from langchain_core.runnables import Runnable
from langchain_core.tools import BaseTool
from langchain_core.messages import (HumanMessage, AIMessage, SystemMessage, BaseMessage)



from langgraph.graph import StateGraph, START, END



from state import State, Conversation, Prompt2JSON, ReAct, default_messages
from const_vars import QUERIES, DEBUG, NAME, COLLECTION_NAME, EMBEDDING_MODEL_NAME, EMBEDDING_MODEL, PERSIS_DIRECTORY, VECTOR_DB, CONVERSATION_2_JSON_MSG_PROMPT, MGR_SYS_MSG_PROMPT, VER_RELEVANCY_MSG_PROMPT, VER_ADEQUACY_MSG_PROMPT, PROMPT_2_JSON_SYS_MSG_PROMPT, RAP_SYS_MSG_PROMPT, SPECIAL_TOKENS_LLAMA_MODELS, CONFIG, CHECKPOINTER, STORE, LLM_HTEMP, LLM_LTEMP, LLM_STRUC_OUT_AUTOML, LLM_STRUC_OUT_CONVERSATION
from nodes import manager_agent, request_verify, prompt_agent, retrieval_augmented_planning_agent, data_agent, model_agent
from auto_cot.api import cot



# TODO: T√≠ch h·ª£p auto-cot.

# TODO: Tr∆∞·ªõc ti√™n c·∫ßn t·ªëi ∆∞u prompt.

# TODO: Build RAG pipeline.

# TODO:  `*state["messages"],` ƒë·ªÉ ƒë∆∞a ng·ªØ c·∫£nh l·ªãch s·ª≠ tr√≤ chuy·ªán v√†o m√¥ h√¨nh.

# TODO: √ù t∆∞·ªüng s·ª≠ d·ª•ng Multi-Agent g·ªçi ƒë·∫øn Yolov8 API, Yolov8 
# API s·∫Ω l·∫•y m·ªçi h√¨nh ·∫£nh c·ª° nh·ªè n√≥ ph√°t hi·ªán  ƒë∆∞·ª£c l√† l·ªói v√† 
# ƒë∆∞a v√†o m√¥ h√¨nh llama3.2-11b-vision ƒë·ªÉ ƒë·ªçc ·∫£nh, sau ƒë√≥ 
# Llama3.2-11b-vision g·ª≠i l·∫°i text ƒë·∫øn Multi-Agent ƒë·ªÉ 
# Multi-Agent x√°c ƒë·ªãnh xem ƒë·∫•y c√≥ ph·∫£i l√† l·ªói kh√¥ng.

# N·∫øu mu·ªën v·∫≠y th√¨ hi·ªán t·∫°i ta c·∫ßn c√≥ data-agent v√† model-agent
# data-agent ƒë·ªÉ generate d·ªØ li·ªáu training, model-agent ƒë·ªÉ vi·∫øt 
# ki·∫øn tr√∫c model vision.

# B√¢y gi·ªù c·∫ßn build tools tr∆∞·ªõc cho m√¥ h√¨nh, bao g·ªìm 
# tool vision, tool genData, tool llama3.2-11b-vision-instruct

# Nh∆∞ng t·∫°i sao l·∫°i ko d√πng vision yolov8 ƒë·ªÉ finetune. M·ª•c ti√™u 
# ·ªü ƒë√¢y, ta s·∫Ω t·∫≠n d·ª•ng c·∫£ s·ª©c m·∫°nh c·ªßa LLM-Vision, ƒë·ªÉ hi·ªÉu r√µ
# h√¨nh ·∫£nh c√≥ g√¨, sau ƒë√≥ g·ª≠i v·ªÅ cho LLM-instruct ƒë·ªÉ x·ª≠ l√Ω text 
# t·ª´ LLM-vision.

# **1. Image-to-Image Generation (CycleGAN, Pix2Pix, StyleGAN)**
# **2. Data Augmentation (Bi·∫øn ƒë·ªïi d·ªØ li·ªáu)**

# TODO: L·∫•y tr·∫°ng th√°i, l·ªãch s·ª≠ b·∫±ng c√°ch `app.get_state(CONFIG).values`

# TODO: Working on this, build integrate auto chain of thought to multi-agent
# x = cot(method="auto_cot", question="", debug=False)

# logging.basicConfig(level=logging.CRITICAL)
workflow = StateGraph(State)

workflow.add_node("MANAGER_AGENT", manager_agent)
workflow.add_node("REQUEST_VERIFY", request_verify)
workflow.add_node("PROMPT_AGENT", prompt_agent)
workflow.add_node("RAP", retrieval_augmented_planning_agent)
workflow.add_node("DATA_AGENT", data_agent)
workflow.add_node("MODEL_AGENT", model_agent)

workflow.add_edge(START, "MANAGER_AGENT")
workflow.add_edge("MANAGER_AGENT", "REQUEST_VERIFY")
workflow.add_conditional_edges("REQUEST_VERIFY", request_verify, ["PROMPT_AGENT", END])
workflow.add_edge("PROMPT_AGENT", END)


app = workflow.compile(checkpointer=CHECKPOINTER, store=STORE, debug=DEBUG, name=NAME)



def main() -> None:
	"""X·ª≠ l√Ω truy v·∫•n c·ªßa ng∆∞·ªùi d√πng v√† hi·ªÉn th·ªã ph·∫£n h·ªìi t·ª´ AI.

	Workflow:
		1. Nh·∫≠n truy v·∫•n t·ª´ danh s√°ch `QUERIES`.
		2. Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ nh·∫≠p "exit" ƒë·ªÉ tho√°t kh√¥ng.
		3. G·ª≠i truy v·∫•n ƒë·∫øn h·ªá th·ªëng AI th√¥ng qua `app.invoke()`.
		4. Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa d·ªØ li·ªáu tr·∫£ v·ªÅ t·ª´ `app.invoke()`.
		5. Tr√≠ch xu·∫•t `messages` v√† hi·ªÉn th·ªã k·∫øt qu·∫£ h·ªôi tho·∫°i.

	Raises:
		ValueError: N·∫øu `app.invoke()` kh√¥ng tr·∫£ v·ªÅ dictionary ho·∫∑c kh√¥ng ch·ª©a key "messages".
	"""
	for i, user_query in enumerate(QUERIES, 1):
		print(f"\nüë®_query_{i}:")
		print(user_query)
		print("\nü§ñ_response:")
		user_query = user_query.strip()
		if user_query.lower() == "exit":
			print("\n>>> [System Exit] Goodbye! Have a great day! üòä\n")
			break
		state_data = app.invoke(
			input={
				"human_query": [HumanMessage(user_query)], 
				"messages": default_messages()}, 
			config=CONFIG)
		if not isinstance(state_data, dict): raise ValueError("[ERROR]: app.invoke() kh√¥ng tr·∫£ v·ªÅ dictionary.")
		if "messages" not in state_data: raise ValueError("[ERROR]: Key 'messages' kh√¥ng c√≥ trong k·∫øt qu·∫£.")
		messages = state_data["messages"]
		print("\n===== [CONVERSATION RESULTS] =====\n")
		display_conversation_results(messages)
		print("\n===== [END OF CONVERSATION] =====\n")



def display_conversation_results(messages: dict) -> None:
	"""Hi·ªÉn th·ªã k·∫øt qu·∫£ h·ªôi tho·∫°i t·ª´ t·∫•t c·∫£ c√°c agent trong h·ªá th·ªëng.

	Args:
		messages (dict): Dictionary ch·ª©a c√°c tin nh·∫Øn ƒë∆∞·ª£c nh√≥m theo agent v√† lo·∫°i tin nh·∫Øn.
			- **Node**: T√™n agent (v√≠ d·ª•: "MANAGER_AGENT", "PROMPT_AGENT").
			- **Msgs_type**: Dictionary ch·ª©a c√°c lo·∫°i tin nh·∫Øn ("HUMAN", "AI", "SYS"), m·ªói lo·∫°i l√† m·ªôt danh s√°ch tin nh·∫Øn.

	Example:
		messages = {
			"MANAGER_AGENT": {
				"SYS": [],
				"HUMAN": [HumanMessage(content="Hello!")],
				"AI": [AIMessage(content="Hi! How can I assist you?")]
			}
		}

	Returns:
		None: H√†m ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ tr√™n terminal m√† kh√¥ng tr·∫£ v·ªÅ gi√° tr·ªã.
	"""
	if not messages:
		print("[INFO]: Kh√¥ng c√≥ tin nh·∫Øn n√†o trong h·ªôi tho·∫°i.")
		return
	for node, msgs in messages.items():
		print(f"\n[{node}]")
		if isinstance(msgs, dict):
			for msg_category, msg_list in msgs.items():
				if msg_list:
					print(f"  {msg_category}:")
					for msg in msg_list:
						content = getattr(msg, "content", "[No content]")
						print(f"\t- {content}")
		else:
			raise ValueError(f"`msgs` ph·∫£i l√† m·ªôt dictionary ch·ª©a danh s√°ch tin nh·∫Øn, `msgs` hi·ªán t·∫°i l√†: {msgs}")



if __name__ == "__main__":
	fire.Fire(main)
