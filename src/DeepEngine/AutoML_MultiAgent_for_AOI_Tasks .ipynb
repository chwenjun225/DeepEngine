{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "import logfire \n",
    "from pydantic import BaseModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import random\n",
    "import tqdm\n",
    "import requests\n",
    "import json\n",
    "import json5\n",
    "import fire \n",
    "import streamlit as st \n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import (Annotated, TypedDict, Sequence, Union, Optional, Literal, List, Dict, Iterator, Any)\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.tools import InjectedToolCallId, BaseTool\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import (HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.managed import IsLastStep\n",
    "from langgraph.graph import (MessagesState, StateGraph, START, END)\n",
    "from langgraph.prebuilt import (create_react_agent, ToolNode, tools_condition)\n",
    "\n",
    "\n",
    "\n",
    "from tools import (add, subtract, multiply, divide, power, square_root)\n",
    "from prompts_lib import Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_TEMP_MODEL = ChatOllama(model=\"deepseek-r1:latest\", temperature=0.1, num_predict=128_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "\n",
    "class User(BaseModel):\n",
    "\tintent: str = Field(\n",
    "\t\t..., description=\"Action the user wants to perform, e.g., 'build', 'train', 'evaluate'\")\n",
    "\texpertise: str = Field(\n",
    "\t\t..., description=\"User's expertise level, e.g., 'beginner', 'medium', 'expert'\")\n",
    "\n",
    "class Problem(BaseModel):\n",
    "\tarea: str\n",
    "\tdownstream_task: str\n",
    "\tapplication_domain: str\n",
    "\tdescription: str\n",
    "\tperformance_metrics: List[str] = []\n",
    "\tcomplexity_metrics: List[str] = []\n",
    "\n",
    "class Dataset(BaseModel):\n",
    "\tname: str\n",
    "\tmodality: List[str]\n",
    "\ttarget_variables: List[str]\n",
    "\tspecification: Optional[str] = None\n",
    "\tdescription: str\n",
    "\tpreprocessing: List[str] = []\n",
    "\taugmentation: List[str] = []\n",
    "\tvisualization: List[str] = []\n",
    "\tsource: str = Field(\n",
    "\t\t..., description=\"'user-upload', 'ai-generate'\")\n",
    "\n",
    "class Model(BaseModel):\n",
    "\tname: str\n",
    "\tfamily: str \n",
    "\ttype: str = Field(..., description=\"classical machine learning\")\n",
    "\tspecification: Optional[str] = None\n",
    "\n",
    "class ParsedJSON(BaseModel):\n",
    "\tuser: User\n",
    "\tproblem: Problem\n",
    "\tdataset: List[Dataset]\n",
    "\tmodel: Model \n",
    "\n",
    "def validate_json(data):\n",
    "\ttry:\n",
    "\t\tvalidated_data = ParsedJSON(**data)\n",
    "\t\tprint(\">>> JSON hợp lệ!\")\n",
    "\t\treturn validated_data\n",
    "\texcept ValidationError as e:\n",
    "\t\tprint(\">>> JSON không hợp lệ:\", e)\n",
    "\t\treturn None\n",
    "\n",
    "input_json = {\n",
    "\t\"user\": {\"intent\": \"build\", \"expertise\": \"medium\"},\n",
    "\t\"problem\": {\n",
    "\t\t\"area\": \"tabular data analysis\",\n",
    "\t\t\"downstream_task\": \"tabular classification\",\n",
    "\t\t\"application_domain\": \"agriculture\",\n",
    "\t\t\"description\": \"Build a model to classify banana quality...\",\n",
    "\t\t\"performance_metrics\": [],\n",
    "\t\t\"complexity_metrics\": []\n",
    "\t},\n",
    "\t\"dataset\": [\n",
    "\t\t{\n",
    "\t\t\t\"name\": \"banana_quality\",\n",
    "\t\t\t\"modality\": [\"tabular\"],\n",
    "\t\t\t\"target_variables\": [\"quality\"],\n",
    "\t\t\t\"specification\": None,\n",
    "\t\t\t\"description\": \"A dataset containing numerical information about bananas...\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "\n",
    "validate_json(input_json) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [HumanMessage(\"What is the capital of France?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(\"\"\"You are a helpful assistant that responds to questions with three exclamation marks.\"\"\")\n",
    "human_msg = HumanMessage('What is the capital of France?')\n",
    "model.invoke([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "template.invoke({\"context\": \"\"\"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\"\"\",\n",
    "\t\"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class AnswerWithJustification(BaseModel):\n",
    "\t'''An answer to the user's question along with justification for the answer.'''\n",
    "\tanswer: str\n",
    "\t'''The answer to the user's question'''\n",
    "\tjustification: str\n",
    "\t'''Justification for the answer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
    "structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
