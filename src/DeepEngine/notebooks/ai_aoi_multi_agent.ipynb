{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML-MultiAgent for AOI Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import random\n",
    "import tqdm\n",
    "import requests\n",
    "import json\n",
    "import json5\n",
    "import fire \n",
    "import streamlit as st \n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, TypeAdapter\n",
    "from typing_extensions import (Annotated, TypedDict, Sequence, Union, Optional, Literal, List, Dict, Iterator, Any, Type)\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.tools import InjectedToolCallId, BaseTool\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import (HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.managed import IsLastStep\n",
    "from langgraph.graph import (MessagesState, StateGraph, START, END)\n",
    "from langgraph.prebuilt import (create_react_agent, ToolNode, tools_condition)\n",
    "\n",
    "\n",
    "\n",
    "from tools import (add, subtract, multiply, divide, power, square_root)\n",
    "from prompts import Prompts \n",
    "\n",
    "\n",
    "\n",
    "NAME = \"FOXCONN-AI Research\"\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "\n",
    "BEGIN_OF_TEXT\t\t=\t\"<|begin_of_text|>\"\n",
    "END_OF_TEXT\t\t\t= \t\"<|end_of_text|>\"\n",
    "START_HEADER_ID\t\t= \t\"<|start_header_id|>\"\n",
    "END_HEADER_ID\t\t= \t\"<|end_header_id|>\"\n",
    "END_OF_MESSAGE_ID\t= \t\"<|eom_id|>\"\n",
    "END_OF_TURN_ID\t\t= \t\"<|eot_id|>\"\n",
    "\n",
    "\n",
    "\n",
    "MSG_TYPES = {\tSystemMessage: \"SYS\", HumanMessage: \"HUMAN\", AIMessage: \"AI\"\t}\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_AGENTS: Dict[str, Dict[str, List[BaseMessage]]] = {\n",
    "\t\"MANAGER_AGENT\": \t{\t\"SYS\": [], \"HUMAN\": [], \"AI\": []\t},\n",
    "\t\"REQUEST_VERIFY\": \t{\t\"SYS\": [], \"HUMAN\": [], \"AI\": []\t},\n",
    "\t\"PROMPT_AGENT\": \t{\t\"SYS\": [], \"HUMAN\": [], \"AI\": []\t},\n",
    "\t\"DATA_AGENT\": \t\t{\t\"SYS\": [], \"HUMAN\": [], \"AI\": []\t},\n",
    "\t\"MODEL_AGENT\": \t\t{\t\"SYS\": [], \"HUMAN\": [], \"AI\": []\t},\n",
    "\t\"OP_AGENT\": \t\t{\t\"SYS\": [], \"HUMAN\": [], \"AI\": []\t},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def default_messages() -> Dict[str, Dict[str, List[BaseMessage]]]:\n",
    "\t\"\"\"Tạo dictionary mặc định cho `messages`, giữ nguyên danh sách các Agent.\n",
    "\n",
    "\t- Sử dụng `defaultdict` để tránh lỗi KeyError nếu truy cập Agent chưa tồn tại.\n",
    "\t- `lambda: {\"SYS\": [], \"HUMAN\": [], \"AI\": []}` đảm bảo mỗi Agent có đủ 3 loại tin nhắn.\n",
    "\t- `DEFAULT_AGENTS.copy()` giúp giữ nguyên cấu trúc ban đầu mà không bị ghi đè.\n",
    "\n",
    "\tReturns:\n",
    "\t\tDict[str, Dict[str, List[BaseMessage]]]: Cấu trúc lưu trữ tin nhắn theo Agent và loại tin nhắn.\n",
    "\t\"\"\"\n",
    "\treturn defaultdict(lambda: {\"SYS\": [], \"HUMAN\": [], \"AI\": []}, DEFAULT_AGENTS.copy())\n",
    "\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "\t\"\"\"Manages structured conversation state in a multi-agent system.\n",
    "\n",
    "\tAttributes:\n",
    "\t\thuman_query (List[HumanMessage]): List of user queries.\n",
    "\t\tmessages (Dict[str, Dict[str, List[BaseMessage]]]): \n",
    "\t\t\tStores categorized messages by agent type and message type:\n",
    "\t\t\t- **Agent types**: MANAGER_AGENT, REQUEST_VERIFY, PROMPT_AGENT, DATA_AGENT, MODEL_AGENT, OP_AGENT.\n",
    "\t\t\t- **Message types**: SYSTEM, HUMAN, AI.\n",
    "\t\tis_last_step (bool): Indicates if this is the final step.\n",
    "\t\tremaining_steps (int): Number of steps left.\n",
    "\n",
    "\tMethods:\n",
    "\t\tget_all_msgs(): Retrieves all messages across all agents in chronological order.\n",
    "\t\tget_latest_msg(agent_type, msg_type): Returns the latest message from a given agent and type.\n",
    "\t\tget_msgs_by_agent_type_and_msg_type(agent_type, msg_type): Retrieves all messages from a specific agent and type.\n",
    "\t\tadd_unique_msgs(node, msgs_type, msg): Adds a unique message to a specified node.\n",
    "\t\"\"\"\n",
    "\thuman_query: Annotated[List[HumanMessage], add_messages] = Field(default_factory=list)\n",
    "\tmessages: Dict[str, Dict[str, List[BaseMessage]]] = Field(\n",
    "\t\tdefault_factory=default_messages, \n",
    "\t\tdescription=\"Categorized messages by agent type and message type.\"\n",
    "\t)\n",
    "\tis_last_step: bool = False\n",
    "\tremaining_steps: int = 3\n",
    "\n",
    "\tdef get_all_msgs(self) -> List[BaseMessage]:\n",
    "\t\t\"\"\"Retrieves all messages from all agents in chronological order.\"\"\"\n",
    "\t\tall_messages = []\n",
    "\t\tfor agent_messages in self.messages.values():\n",
    "\t\t\tfor msg_list in agent_messages.values():\n",
    "\t\t\t\tall_messages.extend(msg_list)\n",
    "\t\treturn all_messages\n",
    "\n",
    "\tdef get_latest_msg(self, agent_type: str, msg_type: str) -> BaseMessage:\n",
    "\t\t\"\"\"Returns the latest message from a specified agent and type.\n",
    "\n",
    "\t\tRaises:\n",
    "\t\t\tValueError: If the agent type or message type is invalid.\n",
    "\t\t\"\"\"\n",
    "\t\tif agent_type not in self.messages: raise ValueError(f\"[ERROR]: Invalid agent category '{agent_type}'. Must be one of {list(self.messages.keys())}.\")\n",
    "\t\tif msg_type not in self.messages[agent_type]: raise ValueError(f\"[ERROR]: Invalid message type '{msg_type}'. Must be 'SYSTEM', 'HUMAN', or 'AI'.\")\n",
    "\t\treturn self.messages[agent_type][msg_type][-1] if self.messages[agent_type][msg_type] else None\n",
    "\n",
    "\tdef get_msgs_by_node_and_msgs_type(self, node: str, msgs_type: str) -> List[BaseMessage]:\n",
    "\t\t\"\"\"Retrieves all messages from a specified agent and type.\n",
    "\n",
    "\t\tRaises:\n",
    "\t\t\tValueError: If the node or message type is invalid.\n",
    "\t\t\"\"\"\n",
    "\t\tif node not in self.messages: raise ValueError(f\"[ERROR]: Invalid agent category '{node}'. Must be one of {list(self.messages.keys())}.\")\n",
    "\t\tif msgs_type not in self.messages[node]: raise ValueError(f\"[ERROR]: Invalid message type '{msgs_type}'. Must be 'SYSTEM', 'HUMAN', or 'AI'.\")\n",
    "\t\treturn self.messages[node][msgs_type]\n",
    "\n",
    "\tdef add_unique_msgs(self, node: str, msgs_type: str, msg: BaseMessage) -> None:\n",
    "\t\t\"\"\"Adds a unique message to a specific node in the State.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tnode (str): The agent node (e.g., \"MANAGER_AGENT\", \"REQUEST_VERIFY\").\n",
    "\t\t\tmsgs_type (str): The message type (one of \"AI\", \"HUMAN\", \"SYS\").\n",
    "\t\t\tmsg (BaseMessage): The message object to be stored.\n",
    "\n",
    "\t\tRaises:\n",
    "\t\t\tValueError: If the message type is invalid.\n",
    "\t\t\"\"\"\n",
    "\t\tif node not in self.messages: \n",
    "\t\t\tself.messages[node] = {\"SYS\": [], \"HUMAN\": [], \"AI\": []}\n",
    "\t\tif msgs_type not in {\"SYS\", \"HUMAN\", \"AI\"}: \n",
    "\t\t\traise ValueError(f\"[ERROR]: Invalid message type '{msgs_type}'. Must be 'SYS', 'HUMAN', or 'AI'.\")\n",
    "\t\tif node == \"REQUEST_VERIFY\": \n",
    "\t\t\tself.messages[node][msgs_type] = [msg]\n",
    "\t\telse:\n",
    "\t\t\tif msg.content not in {m.content for m in self.messages[node][msgs_type]}:\n",
    "\t\t\t\tself.messages[node][msgs_type].append(msg)\n",
    "\n",
    "\n",
    "# TODO: Cần thêm prompt để hướng dẫn mô hình trả lời tốt hơn, đề xuất sử dụng chain-of-thought prompt.\n",
    "class Conversation(TypedDict):\n",
    "\t\"\"\"You are an AI assistant. Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\tresponse:\t\tAnnotated[str, ..., \"A conversational response to the user's query\"\t\t\t]\n",
    "\tjustification: \tAnnotated[str, ..., \"A brief explanation or reasoning behind the response.\"\t]\n",
    "\n",
    "\n",
    "\n",
    "class Prompt2JSON(TypedDict):\n",
    "\t\"\"\"Parses user requirements related to AI project potential into structured JSON.\"\"\"\n",
    "\tproblem_area: \tAnnotated[str, ..., \"Problem domain (e.g., tabular data analysis).\"\t\t\t]\n",
    "\ttask: \t\t\tAnnotated[str, ..., \"Type of ML task (e.g., classification, regression).\"\t]\n",
    "\tapplication: \tAnnotated[str, ..., \"Application field (e.g., agriculture, healthcare).\"\t]\n",
    "\tdataset_name: \tAnnotated[str, ..., \"Dataset name (e.g., banana_quality).\"\t\t\t\t\t]\n",
    "\tdata_modality: \tAnnotated[List[str], ..., \"Data modality (e.g., ['tabular', 'image']).\"\t\t]\n",
    "\tmodel_name: \tAnnotated[str, ..., \"Model name (e.g., XGBoost, ResNet).\"\t\t\t\t\t]\n",
    "\tmodel_type: \tAnnotated[str, ..., \"Model type (e.g., vision, text, tabular).\"\t\t\t\t]\n",
    "\tcuda: \t\t\tAnnotated[bool, ..., \"Requires CUDA? (True/False).\"\t\t\t\t\t\t\t]\n",
    "\tvram: \t\t\tAnnotated[str, ..., \"GPU's VRAM required (e.g., '6GB').\"\t\t\t\t\t]\n",
    "\tcpu_cores: \t\tAnnotated[int, ..., \"Number of CPU cores required.\"\t\t\t\t\t\t\t]\n",
    "\tram: \t\t\tAnnotated[str, ..., \"RAM required (e.g., '16GB').\"\t\t\t\t\t\t\t]\n",
    "\n",
    "\n",
    "\n",
    "MGR_SYS_MSG_PROMPT \t\t\t\t= \tPrompts.AGENT_MANAGER_PROMPT\n",
    "VER_RELEVANCY_MSG_PROMPT \t\t= \tPrompts.REQUEST_VERIFY_RELEVANCY\n",
    "VER_ADEQUACY_MSG_PROMPT \t\t= \tPrompts.REQUEST_VERIFY_ADEQUACY\n",
    "CONVERSATION_2_JSON_MSG_PROMPT \t= \tPrompts.CONVERSATION_TO_JSON_PROMPT\n",
    "PROMPT_2_JSON_SYS_MSG_PROMPT \t= \tPrompts.PROMPT_AGENT_PROMPT\n",
    "\n",
    "\n",
    "\n",
    "CONFIG = {\"configurable\": {\"thread_id\": str(uuid.uuid4()), \"recursion_limit\": 100, \"timeout\": 60}}\n",
    "CHECKPOINTER = MemorySaver()\n",
    "STORE = InMemoryStore()\n",
    "\n",
    "\n",
    "\n",
    "LLM_HTEMP\t=\tChatOllama(model=\"llama3.2:3b-instruct-fp16\", temperature=0.8, num_predict=128_000)\n",
    "LLM_LTEMP \t= \tChatOllama(model=\"llama3.2:3b-instruct-fp16\", temperature=0, num_predict=128_000)\n",
    "\n",
    "LLM_STRUC_OUT_CONVERSATION \t=\tLLM_HTEMP.with_structured_output(schema=Conversation, method=\"json_schema\")\n",
    "LLM_STRUC_OUT_AUTOML \t\t= \tLLM_HTEMP.with_structured_output(schema=Prompt2JSON, method=\"json_schema\")\n",
    "\n",
    "\n",
    "\n",
    "def add_special_token_to_human_query(human_msg: str) -> str:\n",
    "\t\"\"\"Enhances the human query by formatting it with special tokens of LLama 3 series models.\n",
    "\n",
    "\tArgs:\n",
    "\t\thuman_msg (str): User's query.\n",
    "\n",
    "\tReturns:\n",
    "\t\tformatted_query: A formatted human query wrapped with special tokens.\n",
    "\n",
    "\tExample:\n",
    "\t\t>>> state.user_query = [HumanMessage(content=\"What is AI?\")]\n",
    "\t\t>>> enhance_human_query(state)\n",
    "\t\tformatted_query=\"<|start_header_id|>HUMAN<|end_header_id|>What is AI?<|end_of_turn_id|><|start_header_id|>AI<|end_header_id|>\"\n",
    "\t\"\"\"\n",
    "\tformatted_query = (\n",
    "\t\tf\"{START_HEADER_ID}HUMAN{END_HEADER_ID}\"\n",
    "\t\tf\"{human_msg}{END_OF_TURN_ID}\"\n",
    "\t\tf\"{START_HEADER_ID}AI{END_HEADER_ID}\"\n",
    "\t)\n",
    "\treturn formatted_query\n",
    "\n",
    "\n",
    "\n",
    "def add_eotext_eoturn_to_ai_msg(ai_msg: AIMessage, end_of_turn_id_token: str = END_OF_TURN_ID, end_of_text_token: str = END_OF_TEXT) -> AIMessage:\n",
    "\t\"\"\"Ensures AIMessage content ends with required special tokens.\n",
    "\n",
    "\tThis function appends `<|end_of_text|>` and `<|eot_id|>` at the end of the message content if they are not already present.\n",
    "\n",
    "\tArgs:\n",
    "\t\tai_msg (AIMessage): The AI-generated message.\n",
    "\t\tend_of_text_token (str, optional) = \"<|end_of_text|>\": Special token indicating the end of the text.\n",
    "\t\tend_of_turn_id_token (str, optional) = \"<|eot_id|>\": Special token marking the end of a conversation turn.\n",
    "\n",
    "\tReturns:\n",
    "\t\tAIMessage: The updated AI message with the required tokens.\n",
    "\n",
    "\tExample:\n",
    "\t\t>>> message = AIMessage(content=\"Hello, how can I assist you?\")\n",
    "\t\t>>> add_eotext_eoturn_to_ai_msg(message, \"<|end_of_text|>\", \"<|eot_id|>\")\n",
    "\t\tAIMessage(content=\"Hello, how can I assist you?<|end_of_text|><|eot_id|>\")\n",
    "\t\"\"\"\n",
    "\tcontent = ai_msg.content.strip()\n",
    "\tif not content.endswith(end_of_turn_id_token): content += end_of_turn_id_token\n",
    "\tif not content.endswith(end_of_turn_id_token + end_of_text_token): content = content.replace(end_of_turn_id_token, end_of_turn_id_token + end_of_text_token)\n",
    "\treturn AIMessage(content=content)\n",
    "\n",
    "\n",
    "# TODO: Tối ưu hàm này, cần loại bỏ `.get()` tránh tạo đối tượng không cần thiết.\n",
    "def build_react_sys_msg_prompt(tool_desc_prompt: str, react_prompt: str, tools: List[BaseTool]) -> str:\n",
    "\t\"\"\"Builds a formatted system prompt with tool descriptions.\n",
    "\n",
    "\tArgs:\n",
    "\t\ttool_desc_prompt (PromptTemplate): Template for tool descriptions.\n",
    "\t\treact_prompt (PromptTemplate): Template for constructing the final system prompt.\n",
    "\t\ttools (List[BaseTool]): List of tool objects.\n",
    "\n",
    "\tReturns:\n",
    "\t\tstr: A fully formatted system prompt with tool descriptions.\n",
    "\t\"\"\"\n",
    "\tlist_tool_desc = [\n",
    "\t\ttool_desc_prompt.format(\n",
    "\t\t\tname_for_model=(tool_info := getattr(tool.args_schema, \"model_json_schema\", lambda: {})()).get(\"title\", \"Unknown Tool\"),\n",
    "\t\t\tname_for_human=tool_info.get(\"title\", \"Unknown Tool\"),\n",
    "\t\t\tdescription_for_model=tool_info.get(\"description\", \"No description available.\"),\n",
    "\t\t\ttype=tool_info.get(\"type\", \"N/A\"),\n",
    "\t\t\tproperties=json.dumps(tool_info.get(\"properties\", {}), ensure_ascii=False),\n",
    "\t\t\trequired=json.dumps(tool_info.get(\"required\", []), ensure_ascii=False),\n",
    "\t\t) + \" Format the arguments as a JSON object.\"\n",
    "\t\tfor tool in tools\n",
    "\t]\n",
    "\tprompt = react_prompt.format(\n",
    "\t\tBEGIN_OF_TEXT=BEGIN_OF_TEXT, \n",
    "\t\tSTART_HEADER_ID=START_HEADER_ID, \n",
    "\t\tEND_HEADER_ID=END_HEADER_ID, \n",
    "\t\tEND_OF_TURN_ID=END_OF_TURN_ID, \n",
    "\t\ttools_desc=\"\\n\\n\".join(list_tool_desc), \n",
    "\t\ttools_name=\", \".join(tool.name for tool in tools)\n",
    "\t)\n",
    "\treturn prompt\n",
    "\n",
    "\n",
    "\n",
    "def conversation2json(msg_prompt: str, llm_structure_output: Runnable[LanguageModelInput, Dict | BaseModel], human_msg: HumanMessage, schema: Type[Dict]) -> json:\n",
    "\t\"\"\"Parses user's query into structured JSON for manager_agent.\n",
    "\n",
    "\tArgs:\n",
    "\t\tllm_structure_output (Runnable[LanguageModelInput, Dict | BaseModel]): LLM function to generate structured output.\n",
    "\t\thuman_msg (HumanMessage): User's input message.\n",
    "\t\tschema (Type[Dict]): TypedDict schema to validate JSON.\n",
    "\n",
    "\tReturns:\n",
    "\t\tDict: Validated JSON containing:\n",
    "\t\t\t{\n",
    "\t\t\t\t\"response\": \"A conversational response to the user's query.\",\n",
    "\t\t\t\t\"justification\": \"A brief explanation or reasoning behind the response.\"\n",
    "\t\t\t}\n",
    "\n",
    "\tRaises:\n",
    "\t\tValueError: If the response does not contain valid JSON or is missing required fields.\n",
    "\t\"\"\"\n",
    "\tjson_data = llm_structure_output.invoke([human_msg])\n",
    "\tif not json_data:\n",
    "\t\tjson_schema = json.dumps(TypeAdapter(schema).json_schema()[\"properties\"], indent=2).strip()\n",
    "\t\tsys_msg = SystemMessage(content=msg_prompt.format(\n",
    "\t\t\tBEGIN_OF_TEXT=BEGIN_OF_TEXT, \n",
    "\t\t\tSTART_HEADER_ID=START_HEADER_ID, \n",
    "\t\t\tEND_HEADER_ID=END_HEADER_ID, \n",
    "\t\t\tjson_schema=json_schema, \n",
    "\t\t\thuman_msg=human_msg.content, \n",
    "\t\t\tEND_OF_TURN_ID=END_OF_TURN_ID\n",
    "\t\t))\n",
    "\t\tai_msg_json = LLM_LTEMP.invoke([sys_msg])\n",
    "\t\tpattern = r\"```json\\n(.*?)\\n```\"\n",
    "\t\tmatch = re.search(pattern=pattern, string=ai_msg_json.content, flags=re.DOTALL)\n",
    "\t\tif not match: raise ValueError(\">>> Không tìm thấy JSON hợp lệ trong phản hồi của mô hình.\")\n",
    "\t\tjson_string = match.group(1).strip()\n",
    "\t\ttry:\n",
    "\t\t\tjson_data = json.loads(json_string)\n",
    "\t\t\tif DEBUG: \n",
    "\t\t\t\tprint(\">>> JSON hợp lệ:\")\n",
    "\t\t\t\tprint(json.dumps(json_data, indent=2, ensure_ascii=False))\n",
    "\t\texcept json.JSONDecodeError as e:\n",
    "\t\t\traise ValueError(f\">>> JSON không hợp lệ (DecodeError): {e}\")\n",
    "\tmissing_keys = set(schema.__annotations__.keys()) - json_data.keys()\n",
    "\textra_keys = json_data.keys() - set(schema.__annotations__.keys())\n",
    "\tif missing_keys: raise ValueError(f\">>> JSON thiếu các trường bắt buộc: {missing_keys}\")\n",
    "\tif extra_keys: raise ValueError(f\">>> JSON có các trường không hợp lệ: {extra_keys}\")\n",
    "\treturn json_data\n",
    "\n",
    "\n",
    "\n",
    "def manager_agent(state: State) -> State:\n",
    "\t\"\"\"Manager Agent.\n",
    "\n",
    "\tExample:\n",
    "\t\t>>> Human query: I need a very accurate model to classify images in the \n",
    "\t\t\t\tButterfly Image Classification dataset into their respective \n",
    "\t\t\t\tcategories. The dataset has been uploaded with its label \n",
    "\t\t\t\tinformation in the labels.csv file.\n",
    "\t\t>>> AI response: Here is a sample code that uses the Keras library to develop and train a convolutional neural network (CNN) model for ...\n",
    "\t\"\"\"\n",
    "\tsys_msg = SystemMessage(content=MGR_SYS_MSG_PROMPT.format(\n",
    "\t\t\tBEGIN_OF_TEXT=BEGIN_OF_TEXT, \n",
    "\t\t\tSTART_HEADER_ID=START_HEADER_ID, \n",
    "\t\t\tEND_HEADER_ID=END_HEADER_ID, \n",
    "\t\t\tEND_OF_TURN_ID=END_OF_TURN_ID\n",
    "\t\t))\n",
    "\thuman_msg = HumanMessage(content=add_special_token_to_human_query(human_msg=state.human_query[-1].content if state.human_query else \"\"))\n",
    "\thuman_msg_json = HumanMessage(json.dumps((conversation2json(\n",
    "\t\t\tmsg_prompt=CONVERSATION_2_JSON_MSG_PROMPT, \n",
    "\t\t\tllm_structure_output=LLM_STRUC_OUT_CONVERSATION, \n",
    "\t\t\thuman_msg=human_msg, schema=Conversation\n",
    "\t\t)), indent=2\n",
    "\t))\n",
    "\tai_msg = LLM_LTEMP.invoke([sys_msg, human_msg, human_msg_json])\n",
    "\tif not isinstance(ai_msg, AIMessage): \n",
    "\t\tai_msg = AIMessage(\n",
    "\t\t\tcontent=ai_msg.strip() \n",
    "\t\t\tif isinstance(ai_msg, str) \n",
    "\t\t\telse \"At node_manager_agent, I'm unable to generate a response.\"\n",
    "\t\t)\n",
    "\tai_msg = add_eotext_eoturn_to_ai_msg(ai_msg=ai_msg, end_of_turn_id_token=END_OF_TURN_ID, end_of_text_token=END_OF_TEXT)\n",
    "\tstate.add_unique_msgs(node=\"MANAGER_AGENT\", msgs_type=\"SYS\", msg=sys_msg)\n",
    "\tstate.add_unique_msgs(node=\"MANAGER_AGENT\", msgs_type=\"HUMAN\", msg=human_msg)\n",
    "\tstate.add_unique_msgs(node=\"MANAGER_AGENT\", msgs_type=\"AI\", msg=AIMessage(\"<|parse_json|>\" + human_msg_json.content + \"<|end_parse_json|>\" + ai_msg.content))\n",
    "\treturn state\n",
    "\n",
    "\n",
    "\n",
    "def check_contain_yes_or_no(ai_msg: str) -> str:\n",
    "\t\"\"\"Checks if the AI response contains 'Yes' or 'No'.\"\"\"\n",
    "\tpattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\s*\\n\\s*(yes|no)\\b\"\n",
    "\tmatch = re.search(pattern=pattern, string=ai_msg, flags=re.IGNORECASE)\n",
    "\tif match:\n",
    "\t\treturn match.group(1).upper()\n",
    "\telse:\n",
    "\t\treturn \"[ERROR]: Không tìm thấy 'Yes' hoặc 'No' trong phản hồi AI!\"\n",
    "\n",
    "\n",
    "\n",
    "def relevancy(state: State) -> List[BaseMessage]:\n",
    "\t\"\"\"Check request verification-relevancy of human_query.\"\"\"\n",
    "\thuman_msg = state.human_query[-1]\n",
    "\tsys_msg = SystemMessage(content=VER_RELEVANCY_MSG_PROMPT.format(\n",
    "\t\tinstruction=human_msg.content, \n",
    "\t\tBEGIN_OF_TEXT=BEGIN_OF_TEXT, \n",
    "\t\tSTART_HEADER_ID=START_HEADER_ID, \n",
    "\t\tEND_HEADER_ID=END_HEADER_ID, \n",
    "\t\tEND_OF_TURN_ID=END_OF_TURN_ID\n",
    "\t))\n",
    "\tai_msg = LLM_LTEMP.invoke([sys_msg])\n",
    "\tif not isinstance(ai_msg, AIMessage): \n",
    "\t\tai_msg = AIMessage(content=ai_msg.strip() if isinstance(ai_msg, str) else \"At node_request_verify-REQUEST_VERIFY_RELEVANCY, I'm unable to generate a response.\")\n",
    "\tai_msg = add_eotext_eoturn_to_ai_msg(ai_msg=ai_msg, end_of_turn_id_token=END_OF_TURN_ID, end_of_text_token=END_OF_TEXT)\n",
    "\treturn [sys_msg, human_msg, ai_msg]\n",
    "\n",
    "\n",
    "\n",
    "def adequacy(state: State) -> List[BaseMessage]:\n",
    "\t\"\"\"Check request verification-adequacy of AIMessage response with JSON object.\"\"\"\n",
    "\tpattern = r\"<\\|parse_json\\|>(.*?)<\\|end_parse_json\\|>\"\n",
    "\thuman_msg = state.messages['MANAGER_AGENT']['HUMAN'][-1]\n",
    "\tai_msg = state.messages['MANAGER_AGENT']['AI'][-1]\n",
    "\tjson_obj_from_ai_msg = re.findall(pattern=pattern, string=ai_msg.content, flags=re.DOTALL)[-1]\n",
    "\tsys_msg = SystemMessage(content=VER_ADEQUACY_MSG_PROMPT.format(\n",
    "\t\tBEGIN_OF_TEXT=BEGIN_OF_TEXT, \n",
    "\t\tSTART_HEADER_ID=START_HEADER_ID, \n",
    "\t\tEND_HEADER_ID=END_HEADER_ID, \n",
    "\t\tparsed_user_requirements=json_obj_from_ai_msg, \n",
    "\t\tEND_OF_TURN_ID=END_OF_TURN_ID\n",
    "\t))\n",
    "\tai_msg = LLM_LTEMP.invoke([sys_msg])\n",
    "\tif not isinstance(ai_msg, AIMessage): ai_msg = AIMessage(content=ai_msg.strip() if isinstance(ai_msg, str) else \"At node_request_verify-REQUEST_VERIFY_ADEQUACY, I'm unable to generate a response.\")\n",
    "\tai_msg = add_eotext_eoturn_to_ai_msg(ai_msg=ai_msg, end_of_turn_id_token=END_OF_TURN_ID, end_of_text_token=END_OF_TEXT)\n",
    "\treturn [sys_msg, human_msg, ai_msg]\n",
    "\n",
    "\n",
    "\n",
    "def request_verify(state: State) -> State:\n",
    "\t\"\"\"Request verification output of Agent Manager.\"\"\"\n",
    "\tai_msg_relevancy = relevancy(state=state)[2]\n",
    "\tai_msg_adequacy = adequacy(state=state)[2]\n",
    "\tyes_no_relevancy = check_contain_yes_or_no(ai_msg=ai_msg_relevancy.content)\n",
    "\tyes_no_adequacy  = check_contain_yes_or_no(ai_msg=ai_msg_adequacy.content )\n",
    "\tyes_no = \"YES\" if \"YES\" in (yes_no_relevancy, yes_no_adequacy) else \"NO\"\n",
    "\tai_msg = AIMessage(content=yes_no)\n",
    "\tstate.add_unique_msgs(node=\"REQUEST_VERIFY\", msgs_type=\"AI\", msg=ai_msg)\n",
    "\treturn state\n",
    "\n",
    "\n",
    "\n",
    "def req_ver_yes_or_no_control_flow(state: State) -> State:\n",
    "\t\"\"\"Determines the next step based on the AI response from REQUEST_VERIFY.\n",
    "\n",
    "\tArgs:\n",
    "\t\tstate (State): The current conversation state.\n",
    "\n",
    "\tReturns:\n",
    "\t\tstr: The next agent (\"PROMPT_AGENT\" or END).\n",
    "\n",
    "\tRaises:\n",
    "\t\tValueError: If there is no valid AI response or an unexpected response.\n",
    "\t\"\"\"\n",
    "\tif \"REQUEST_VERIFY\" not in state.messages or \"AI\" not in state.messages[\"REQUEST_VERIFY\"]:\n",
    "\t\traise ValueError(\"[ERROR]: No AI message found in REQUEST_VERIFY.\")\n",
    "\tai_msgs = state.messages[\"REQUEST_VERIFY\"][\"AI\"]\n",
    "\tif not ai_msgs:\n",
    "\t\traise ValueError(\"[ERROR]: AI message list is empty in REQUEST_VERIFY.\")\n",
    "\tai_msg = ai_msgs[-1]\n",
    "\tif not hasattr(ai_msg, \"content\"):\n",
    "\t\traise ValueError(\"[ERROR]: AI message has no content.\")\n",
    "\tresp = ai_msg.content.strip().upper()\n",
    "\tnext_step_map = {\"YES\": \"PROMPT_AGENT\", \"NO\": END}\n",
    "\tif resp in next_step_map:\n",
    "\t\treturn next_step_map[resp]\n",
    "\traise ValueError(f\">>> [ERROR]: Unexpected response '{resp}'\")\n",
    "\n",
    "\n",
    "\n",
    "def prompt_agent(state: State) -> State:\n",
    "\t\"\"\"Prompt Agent parses user's requirements into JSON following a TypedDict schema.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tstate (State): The current state of the conversation.\n",
    "\n",
    "\tReturns:\n",
    "\t\tState: Updated state with the parsed JSON response.\n",
    "\t\"\"\"\n",
    "\tif \"MANAGER_AGENT\" not in state.messages or \"HUMAN\" not in state.messages[\"MANAGER_AGENT\"]:\n",
    "\t\traise ValueError(\"[ERROR]: No HUMAN messages found in MANAGER_AGENT.\")\n",
    "\thuman_msg = state.messages[\"MANAGER_AGENT\"][\"HUMAN\"][-1]\n",
    "\tparsed_json = conversation2json(\n",
    "\t\tmsg_prompt=PROMPT_2_JSON_SYS_MSG_PROMPT, \n",
    "\t\tllm_structure_output=LLM_STRUC_OUT_AUTOML,\n",
    "\t\thuman_msg=human_msg,\n",
    "\t\tschema=Prompt2JSON\n",
    "\t)\n",
    "\texpected_keys, received_keys = set(Prompt2JSON.__annotations__), set(parsed_json)\n",
    "\tif missing_keys := expected_keys - received_keys: \n",
    "\t\traise ValueError(f\"[ERROR]: JSON thiếu các trường bắt buộc: {missing_keys}\")\n",
    "\tif extra_keys := received_keys - expected_keys: \n",
    "\t\traise ValueError(f\"[ERROR]: JSON có các trường không hợp lệ: {extra_keys}\")\n",
    "\tai_msg_json = AIMessage(content=json.dumps(parsed_json, indent=2))\n",
    "\tai_msg_json = add_eotext_eoturn_to_ai_msg(ai_msg=ai_msg_json, end_of_turn_id_token=END_OF_TURN_ID, end_of_text_token=END_OF_TEXT)\n",
    "\tstate.add_unique_msgs(node=\"PROMPT_AGENT\", msgs_type=\"AI\", msg=ai_msg_json)\n",
    "\treturn state\n",
    "\n",
    "\n",
    "\n",
    "def rap_agent(state: State) -> State:\n",
    "\t\"Retrieval-Augmented Planning Agent.\"\n",
    "\treturn state\n",
    "\n",
    "\n",
    "\n",
    "def data_agent(state: State) -> State:\n",
    "\t\"Data Agent.\"\n",
    "\treturn state\n",
    "\n",
    "\n",
    "\n",
    "def model_agent(state: State) -> State:\n",
    "\t\"Model Agent.\"\n",
    "\treturn state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"MANAGER_AGENT\", manager_agent)\n",
    "workflow.add_node(\"REQUEST_VERIFY\", request_verify)\n",
    "workflow.add_node(\"PROMPT_AGENT\", prompt_agent)\n",
    "workflow.add_node(\"RETRIEVAL_AUGMENTED_PLANNING\", rap_agent)\n",
    "workflow.add_node(\"DATA_AGENT\", data_agent)\n",
    "workflow.add_node(\"MODEL_AGENT\", model_agent)\n",
    "\n",
    "workflow.add_edge(START, \"MANAGER_AGENT\")\n",
    "workflow.add_edge(\"MANAGER_AGENT\", \"REQUEST_VERIFY\")\n",
    "workflow.add_conditional_edges(\"REQUEST_VERIFY\", req_ver_yes_or_no_control_flow, [\"PROMPT_AGENT\", END])\n",
    "workflow.add_edge(\"PROMPT_AGENT\", \"RETRIEVAL_AUGMENTED_PLANNING\")\n",
    "workflow.add_edge(\"RETRIEVAL_AUGMENTED_PLANNING\", \"DATA_AGENT\")\n",
    "workflow.add_edge(\"RETRIEVAL_AUGMENTED_PLANNING\", \"MODEL_AGENT\")\n",
    "workflow.add_edge(\"DATA_AGENT\", \"MODEL_AGENT\")\n",
    "workflow.add_edge(\"MODEL_AGENT\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=CHECKPOINTER, store=STORE, debug=DEBUG, name=NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
